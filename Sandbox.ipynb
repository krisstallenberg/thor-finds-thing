{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb8ed39-2cc8-417b-b8c7-35a0758079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64018c5",
   "metadata": {},
   "source": [
    "## Structured output testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7deec3a-7c3b-4709-bf71-49f913e953d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object_description\": {\n",
      "        \"name\": \"knife\",\n",
      "        \"position\": \"on a table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"context_description\": {\n",
      "        \"left_of_object\": \"TV\",\n",
      "        \"right_of_object\": null,\n",
      "        \"behind_object\": \"two windows\",\n",
      "        \"in_front_of_object\": null,\n",
      "        \"above_object\": null,\n",
      "        \"below_object\": null,\n",
      "        \"additional_information\": [\n",
      "            \"The table is far from the wall.\"\n",
      "        ]\n",
      "    },\n",
      "    \"room_description\": {\n",
      "        \"room_type\": null,\n",
      "        \"size\": \"medium sized\",\n",
      "        \"additional_information\": [\n",
      "            \"It appears to be a kitchen or living room.\"\n",
      "        ]\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ContextDescription(BaseModel):\n",
    "    left_of_object: Optional[str]\n",
    "    right_of_object: Optional[str]\n",
    "    behind_object: Optional[str]\n",
    "    in_front_of_object: Optional[str]\n",
    "    above_object: Optional[str]\n",
    "    below_object: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object_description: Optional[ObjectDescription]\n",
    "    context_description: Optional[ContextDescription]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "    \n",
    "user_description= \"\"\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description.\"},\n",
    "        {\"role\": \"user\", \"content\": user_description}\n",
    "    ],\n",
    "    response_format=InitialDescription,\n",
    ")\n",
    "\n",
    "structured_description_struct_output = completion.choices[0].message.parsed\n",
    "print(json.dumps(structured_description_struct_output.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67805a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object_description\": {\n",
      "        \"name\": \"knife\",\n",
      "        \"position\": \"on a table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"context_description\": {\n",
      "        \"left_of_object\": \"TV\",\n",
      "        \"right_of_object\": \"two windows\",\n",
      "        \"behind_object\": \"wall\",\n",
      "        \"in_front_of_object\": null,\n",
      "        \"above_object\": null,\n",
      "        \"below_object\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"room_description\": {\n",
      "        \"room_type\": \"Kitchen\",\n",
      "        \"size\": \"medium sized\",\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ContextDescription(BaseModel):\n",
    "    left_of_object: Optional[str]\n",
    "    right_of_object: Optional[str]\n",
    "    behind_object: Optional[str]\n",
    "    in_front_of_object: Optional[str]\n",
    "    above_object: Optional[str]\n",
    "    below_object: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object_description: Optional[ObjectDescription]\n",
    "    context_description: Optional[ContextDescription]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "llm = OpenAI()\n",
    "\n",
    "chat_prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage.from_str(\n",
    "            \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. Here is the description:\\n {movie_name}\", role=\"user\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_description_llamaindex = llm.structured_predict(\n",
    "    InitialDescription, chat_prompt_tmpl, movie_name=\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\n",
    ")\n",
    "\n",
    "print(json.dumps(structured_description_llamaindex.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91456c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object_description {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None} {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None}\n",
      "context_description {'left_of_object': 'TV', 'right_of_object': None, 'behind_object': 'two windows', 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': ['The table is far from the wall.']} {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': 'wall', 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None}\n",
      "room_description {'room_type': None, 'size': 'medium sized', 'additional_information': ['It appears to be a kitchen or living room.']} {'room_type': 'Kitchen', 'size': 'medium sized', 'additional_information': None}\n",
      "additional_information None None\n"
     ]
    }
   ],
   "source": [
    "# Compare structured_description_llamaindex and structured_description_struct_output by printing the values side by side\n",
    "for openai_key, llama_key in zip (structured_description_struct_output.model_dump().items(), structured_description_llamaindex.dict().items()):\n",
    "    print(openai_key[0], openai_key[1], llama_key[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6827b3",
   "metadata": {},
   "source": [
    "## AI2Thor wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a715eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "import ai2thor\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42fa9180-6728-4c7b-a7cd-b5b11b991c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "class AI2ThorClient: \n",
    "    \"\"\"\n",
    "    An AI2Thor instance with methods wrapping its controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._controller = Controller(\n",
    "            agentMode=\"default\",\n",
    "            visibilityDistance=1.5,\n",
    "            scene=\"FloorPlan212\",\n",
    "\n",
    "            # step sizes\n",
    "            \n",
    "            gridSize=0.25,\n",
    "            snapToGrid=True,\n",
    "            rotateStepDegrees=90,\n",
    "\n",
    "            # image modalities\n",
    "            renderDepthImage=False,\n",
    "            renderInstanceSegmentation=False,\n",
    "\n",
    "            # camera properties\n",
    "            width=512,\n",
    "            height=512,\n",
    "            fieldOfView=90\n",
    "            )\n",
    "        self._metadata = []\n",
    "        self._llm_ollama = OllamaLlamaIndex(model=\"llama3.2\", request_timeout=120.0)\n",
    "        self._llm_openai = OpenAILlamaIndex(model=\"gpt-4o-2024-08-06\")\n",
    "        self._llm_openai_multimodal = OpenAI()\n",
    "        \n",
    "    def _get_image(self):\n",
    "        return Image.fromarray(self._controller.last_event.frame)\n",
    "    \n",
    "    \n",
    "    def describe_scene_from_image(self):\n",
    "        \"\"\"\n",
    "        Describes the scene using an image-to-text model.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A string describing the current scene.\n",
    "        \"\"\"\n",
    "        \n",
    "        image = self._get_image()\n",
    "        \n",
    "        img_path  = f\"log/img/{str(time.time())}.jpg\"\n",
    "        image.save(img_path)\n",
    "        \n",
    "        encoded_image = encode_image(img_path)\n",
    "        \n",
    "        response = self._llm_openai_multimodal.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What is in this image?\",\n",
    "                        },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    \n",
    "    def step(self, direction: str = \"MoveAhead\", magnitude: float = None) -> ai2thor.server.Event:\n",
    "        \"\"\"\n",
    "        Robot takes one step in given direction.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            moveMagnitude=magnitude)\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def look(self, direction: str = \"LookUp\") -> None:\n",
    "        \"\"\"\n",
    "        Robot looks up or down.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=30\n",
    "            )\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def rotate(self, direction: str, degrees: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot turns in given direction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        direction : str\n",
    "            Direction to turn in. Can be \"RotateLeft\" or \"RotateRight\".\n",
    "        \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=degrees\n",
    "            )\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def crouch(self):\n",
    "        \"\"\"\n",
    "        Robot crouches.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Crouch\")\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def stand(self):\n",
    "        \"\"\"\n",
    "        Robot stands.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Stand\")\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def teleport(self, position: dict = None, rotation: dict = None, horizon: float = None, standing: bool = None, to_random: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Robot teleports to random location.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        \n",
    "        if to_random:\n",
    "            rotation = {\"x\": random.randint(0, 360), \"y\": random.randint(0, 360), \"z\": random.randint(0, 360)}\n",
    "            positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "            position = random.choice(positions)\n",
    "            \n",
    "        self._controller.step(\n",
    "            action=\"Teleport\",\n",
    "            position=position,\n",
    "            rotation=rotation,\n",
    "            horizon=horizon,\n",
    "            standing=standing\n",
    "        )\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def done(self) -> None:\n",
    "        \"\"\"\n",
    "        The Done action does nothing to the state of the environment. \n",
    "        But, it returns a cleaned up event with respect to the metadata.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Done\")\n",
    "        \n",
    "        self._metadata.append(self._controller.last_event.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "494431fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor = AI2ThorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5297c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = thor.teleport(to_random=True)\n",
    "event = thor.rotate(direction=\"RotateRight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43fa3543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The image shows an interior space with wooden flooring. There are several doors visible, each with white paneling and brass handles. In the foreground on the left, there's an open cardboard box on a red surface, and nearby there is an object that resembles a shoehorn. The walls are painted in light colors, contributing to a clean and minimalist appearance.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor.describe_scene_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts an interior space that appears to be a room or hallway. There are walls painted in different colors, with a light-colored wall on the left and a darker wall on the right. There are several closed doors visible, one of which is white. In the foreground, there is a wooden table or shelf with a cardboard box on it and what looks like a shoe or object next to the box. The flooring seems to be wooden. The overall atmosphere suggests a simple, possibly minimalistic interior design.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "from llama_index.core.schema import ImageDocument\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "image = thor._get_image()\n",
    "img_path  = f\"{str(time.time())}.jpg\"\n",
    "image.save(img_path)\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "encoded_image = encode_image(img_path)\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\",\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb43c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='I don\\'t have a physical presence, so I don\\'t see things like humans do. However, I can process and analyze vast amounts of information that is available to me through various inputs such as text data, images, or audio files.\\n\\nIn terms of visual perception, I can:\\n\\n1. **Recognize text**: I can understand and process written text, including emails, articles, documents, and more.\\n2. **Process images**: While I don\\'t \"see\" in the classical sense, I can analyze and understand images, including recognizing objects, scenes, and activities.\\n3. **Understand audio**: I can transcribe and interpret spoken language from various sources like podcasts, audiobooks, or voice messages.\\n\\nPlease keep in mind that my perception is limited to text-based data and doesn\\'t involve direct sensory inputs like sight, sound, touch, taste, or smell.\\n\\nHow can I assist you today?', additional_kwargs={'tool_calls': []}, raw={'model': 'llama3.2', 'created_at': '2024-11-10T16:18:07.337071Z', 'message': {'role': 'assistant', 'content': 'I don\\'t have a physical presence, so I don\\'t see things like humans do. However, I can process and analyze vast amounts of information that is available to me through various inputs such as text data, images, or audio files.\\n\\nIn terms of visual perception, I can:\\n\\n1. **Recognize text**: I can understand and process written text, including emails, articles, documents, and more.\\n2. **Process images**: While I don\\'t \"see\" in the classical sense, I can analyze and understand images, including recognizing objects, scenes, and activities.\\n3. **Understand audio**: I can transcribe and interpret spoken language from various sources like podcasts, audiobooks, or voice messages.\\n\\nPlease keep in mind that my perception is limited to text-based data and doesn\\'t involve direct sensory inputs like sight, sound, touch, taste, or smell.\\n\\nHow can I assist you today?'}, 'done_reason': 'stop', 'done': True, 'total_duration': 4463276666, 'load_duration': 28935125, 'prompt_eval_count': 30, 'prompt_eval_duration': 599843000, 'eval_count': 185, 'eval_duration': 3833914000, 'usage': {'prompt_tokens': 30, 'completion_tokens': 185, 'total_tokens': 215}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor._llm_ollama.complete(\"What do you see?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcba9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcbb1090",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d1dca3b-b338-4146-b079-0586c0653dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thor: Tell me what you saw in detail. Describe the object, its context and the type of room you saw.\n",
      "Initial description is incomplete.\n",
      "Initial description clarified.\n",
      "Teleported to correct room type.\n",
      "Object is not in this room.\n",
      "Teleported to correct room type.\n",
      "Object is not in this room.\n",
      "Teleported to incorrect room type.\n",
      "Teleported to correct room type.\n",
      "Object is not in this room.\n",
      "Teleported to correct room type.\n",
      "Object is not in this room.\n",
      "Teleported to incorrect room type.\n",
      "Teleported to incorrect room type.\n",
      "Teleported to correct room type.\n",
      "Object may be in this room.\n",
      "Teleported to correct room type.\n",
      "Object may be in this room.\n",
      "Wrong object suggested.\n",
      "Teleported to correct room type.\n",
      "Object is not in this room.\n",
      "Teleported to correct room type.\n",
      "Object may be in this room.\n",
      "Wrong object suggested.\n",
      "Teleported to correct room type.\n",
      "Object may be in this room.\n",
      "Wrong object suggested.\n",
      "Wrong object suggested.\n",
      "Wrong object suggested.\n",
      "We found the object!\n"
     ]
    }
   ],
   "source": [
    "class InitialDescriptionComplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class InitialDescriptionIncomplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectFound(Event):\n",
    "    payload: str\n",
    "\n",
    "class WrongObjectSuggested(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomCorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomIncorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectNotInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "number = 1\n",
    "\n",
    "class ThorFindsObject(Workflow):\n",
    "    \n",
    "    @step\n",
    "    async def ask_initial_description(self, ev: StartEvent) -> InitialDescriptionComplete | InitialDescriptionIncomplete:\n",
    "        print(\"Thor: Tell me what you saw in detail. Describe the object, its context and the type of room you saw.\")\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Initial description is complete.\")\n",
    "            return InitialDescriptionComplete(payload=\"Initial description is complete.\")\n",
    "        else:\n",
    "            print(\"Initial description is incomplete.\")\n",
    "            return InitialDescriptionIncomplete(payload=\"Initial description is incomplete.\")\n",
    "\n",
    "    @step\n",
    "    async def clarify_initial_description(self, ev: InitialDescriptionIncomplete) -> InitialDescriptionComplete:\n",
    "        print(\"Initial description clarified.\")\n",
    "        return InitialDescriptionComplete(payload=\"Description clarified.\")\n",
    "\n",
    "    @step\n",
    "    async def random_teleport(self, ev: InitialDescriptionComplete | RoomIncorrect | ObjectNotInRoom) -> RoomCorrect | RoomIncorrect:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Teleported to correct room type.\")\n",
    "            return RoomCorrect(payload=\"Correct room is found.\")\n",
    "        else:\n",
    "            print(\"Teleported to incorrect room type.\")\n",
    "            return RoomIncorrect(payload=\"Correct room is not found.\")\n",
    "\n",
    "    @step \n",
    "    async def find_object_in_room(self, ev: RoomCorrect) -> ObjectInRoom | ObjectNotInRoom:\n",
    "        if random.randint(0, 10) < 4:\n",
    "            print(\"Object may be in this room.\")\n",
    "            return ObjectInRoom(payload=\"Object may be in this room.\")\n",
    "        else:\n",
    "            print(\"Object is not in this room.\")\n",
    "            return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "    \n",
    "    @step \n",
    "    async def suggest_object(self, ev: ObjectInRoom | WrongObjectSuggested) -> WrongObjectSuggested | ObjectNotInRoom | StopEvent:\n",
    "        if random.randint(0, 10) < 8:\n",
    "            print(\"Wrong object suggested.\")\n",
    "            return WrongObjectSuggested(payload=\"Couldn't find object in this room.\")\n",
    "        elif random.randint(0, 10) < 4:\n",
    "            return StopEvent(result=\"We found the object!\")  # End the workflow\n",
    "        else:\n",
    "            return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "\n",
    "# Initialize and run the workflow\n",
    "w = ThorFindsObject(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81d8f70-1c60-4eba-87b3-daf0233f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionIncomplete'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.ObjectInRoom'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class '__main__.RoomCorrect'>\n",
      "<class '__main__.RoomIncorrect'>\n",
      "<class '__main__.WrongObjectSuggested'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "possible_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(ThorFindsObject, filename=\"possible_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bbad389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chainlit\n",
      "  Downloading chainlit-1.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiofiles<24.0.0,>=23.1.0 (from chainlit)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting asyncer<0.0.8,>=0.0.7 (from chainlit)\n",
      "  Downloading asyncer-0.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: dataclasses_json<0.7.0,>=0.6.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.6.7)\n",
      "Collecting fastapi<0.116,>=0.115.3 (from chainlit)\n",
      "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from chainlit)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.27.2)\n",
      "Collecting lazify<0.5.0,>=0.4.0 (from chainlit)\n",
      "  Using cached Lazify-0.4.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting literalai==0.0.623 (from chainlit)\n",
      "  Downloading literalai-0.0.623.tar.gz (57 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.26.4)\n",
      "Collecting packaging<24.0,>=23.1 (from chainlit)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.0.1)\n",
      "Collecting python-multipart<0.0.10,>=0.0.9 (from chainlit)\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-socketio<6.0.0,>=5.11.0 (from chainlit)\n",
      "  Downloading python_socketio-5.11.4-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting starlette<0.42.0,>=0.41.2 (from chainlit)\n",
      "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting syncer<3.0.0,>=2.0.3 (from chainlit)\n",
      "  Using cached syncer-2.0.3.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tomli<3.0.0,>=2.0.1 (from chainlit)\n",
      "  Downloading tomli-2.0.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting uptrace<2.0.0,>=1.22.0 (from chainlit)\n",
      "  Downloading uptrace-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uvicorn<0.26.0,>=0.25.0 (from chainlit)\n",
      "  Using cached uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting watchfiles<0.21.0,>=0.20.0 (from chainlit)\n",
      "  Using cached watchfiles-0.20.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting chevron>=0.14.0 (from literalai==0.0.623->chainlit)\n",
      "  Using cached chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in ./myenv/lib/python3.11/site-packages (from asyncer<0.0.8,>=0.0.7->chainlit) (4.6.2.post1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.11/site-packages (from fastapi<0.116,>=0.115.3->chainlit) (4.12.2)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.0.6)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (3.10)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (2.23.4)\n",
      "Collecting bidict>=0.21.0 (from python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting python-engineio>=4.8.0 (from python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Downloading python_engineio-4.10.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-api~=1.27 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp~=1.27 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_exporter_otlp-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation~=0.48b0 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-sdk~=1.27 (from uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\n",
      "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.28.0 (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.28.0 (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpcio<2.0.0,>=1.63.2 (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading grpcio-1.67.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: requests~=2.7 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.3)\n",
      "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.0->opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.16.0)\n",
      "Collecting simple-websocket>=0.10.0 (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./myenv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.7.0,>=0.6.7->chainlit) (1.0.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.3)\n",
      "Downloading chainlit-1.3.2-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading asyncer-0.0.7-py3-none-any.whl (8.5 kB)\n",
      "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached Lazify-0.4.0-py2.py3-none-any.whl (3.1 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading python_socketio-5.11.4-py3-none-any.whl (76 kB)\n",
      "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
      "Downloading tomli-2.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading uptrace-1.27.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "Using cached watchfiles-0.20.0-cp37-abi3-macosx_11_0_arm64.whl (407 kB)\n",
      "Using cached bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Using cached chevron-0.14.0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_api-1.28.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.28.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.28.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.28.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl (159 kB)\n",
      "Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl (118 kB)\n",
      "Downloading python_engineio-4.10.1-py3-none-any.whl (57 kB)\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Downloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
      "Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading grpcio-1.67.1-cp311-cp311-macosx_10_9_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Building wheels for collected packages: literalai, syncer\n",
      "  Building wheel for literalai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for literalai: filename=literalai-0.0.623-py3-none-any.whl size=73990 sha256=4a1069666fc5be1574fd4bc53212d2f71a08304e962da4260a7486e9d066dfa0\n",
      "  Stored in directory: /Users/krisstallenberg/Library/Caches/pip/wheels/e6/fe/4a/322b127ccae00d128916f62485eb65e2027a2a1d31484cdc88\n",
      "  Building wheel for syncer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for syncer: filename=syncer-2.0.3-py2.py3-none-any.whl size=3434 sha256=cc75559ac38f9c1ca69aa6ee87e86314504a361418c3c146589a65359926c7c4\n",
      "  Stored in directory: /Users/krisstallenberg/Library/Caches/pip/wheels/fa/a7/d4/9253c7f0256e67e3ff75d4c095c076a3882e319728e94bc6a1\n",
      "Successfully built literalai syncer\n",
      "Installing collected packages: syncer, lazify, filetype, chevron, zipp, wsproto, uvicorn, tomli, python-multipart, protobuf, packaging, grpcio, bidict, aiofiles, watchfiles, starlette, simple-websocket, opentelemetry-proto, importlib-metadata, googleapis-common-protos, asyncer, python-engineio, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, literalai, fastapi, python-socketio, opentelemetry-semantic-conventions, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, uptrace, chainlit\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "Successfully installed aiofiles-23.2.1 asyncer-0.0.7 bidict-0.23.1 chainlit-1.3.2 chevron-0.14.0 fastapi-0.115.4 filetype-1.2.0 googleapis-common-protos-1.65.0 grpcio-1.67.1 importlib-metadata-8.5.0 lazify-0.4.0 literalai-0.0.623 opentelemetry-api-1.28.0 opentelemetry-exporter-otlp-1.28.0 opentelemetry-exporter-otlp-proto-common-1.28.0 opentelemetry-exporter-otlp-proto-grpc-1.28.0 opentelemetry-exporter-otlp-proto-http-1.28.0 opentelemetry-instrumentation-0.49b0 opentelemetry-proto-1.28.0 opentelemetry-sdk-1.28.0 opentelemetry-semantic-conventions-0.49b0 packaging-23.2 protobuf-5.28.3 python-engineio-4.10.1 python-multipart-0.0.9 python-socketio-5.11.4 simple-websocket-1.1.0 starlette-0.41.2 syncer-2.0.3 tomli-2.0.2 uptrace-1.27.0 uvicorn-0.25.0 watchfiles-0.20.0 wsproto-1.2.0 zipp-3.20.2\n"
     ]
    }
   ],
   "source": [
    "!pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d5f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
