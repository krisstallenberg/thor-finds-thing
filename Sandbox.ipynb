{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb8ed39-2cc8-417b-b8c7-35a0758079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64018c5",
   "metadata": {},
   "source": [
    "## Structured output testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7deec3a-7c3b-4709-bf71-49f913e953d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:48:02.803924Z",
     "start_time": "2024-12-06T08:47:57.334383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object\": {\n",
      "        \"name\": \"Knife\",\n",
      "        \"position\": \"On the table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"object_in_context\": [\n",
      "        {\n",
      "            \"name\": \"Table\",\n",
      "            \"position\": \"Far from the wall\",\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": null,\n",
      "            \"position_relative_to_target_object\": \"The knife is on the table\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"TV\",\n",
      "            \"position\": null,\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": null,\n",
      "            \"position_relative_to_target_object\": \"Left of the table which the knife is on\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Windows\",\n",
      "            \"position\": \"On the wall behind the table\",\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": [\n",
      "                \"There are two windows\"\n",
      "            ],\n",
      "            \"position_relative_to_target_object\": \"Right of the table which the knife is on\"\n",
      "        }\n",
      "    ],\n",
      "    \"room_description\": {\n",
      "        \"room_type\": null,\n",
      "        \"size\": \"Medium\",\n",
      "        \"additional_information\": [\n",
      "            \"It appears to be a kitchen or living room\"\n",
      "        ]\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "class ContextObjectDescription(ObjectDescription):\n",
    "    position_relative_to_target_object: Optional[str]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object: Optional[ObjectDescription]\n",
    "    object_in_context: Optional[List[ContextObjectDescription]]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "    \n",
    "user_description= \"\"\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description.\"},\n",
    "        {\"role\": \"user\", \"content\": user_description}\n",
    "    ],\n",
    "    response_format=InitialDescription,\n",
    ")\n",
    "\n",
    "structured_description_struct_output = completion.choices[0].message.parsed\n",
    "print(json.dumps(structured_description_struct_output.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67805a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:48:05.397754Z",
     "start_time": "2024-12-06T08:48:03.011395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object=ObjectDescription(name='knife', position='on a table', size='N/A', texture='N/A', color='N/A', additional_information=[]) object_in_context=[ContextObjectDescription(name='TV', position='left of the table', size='N/A', texture='N/A', color='N/A', additional_information=None, position_relative_to_target_object=None), ContextObjectDescription(name='windows', position='on the wall behind the table', size='N/A', texture='N/A', color='N/A', additional_information=None, position_relative_to_target_object=None)] room_description=RoomDescription(room_type='Kitchen', size='medium sized', additional_information=[]) additional_information=[]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "class ContextObjectDescription(ObjectDescription):\n",
    "    position_relative_to_target_object: Optional[str]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object: Optional[ObjectDescription]\n",
    "    object_in_context: Optional[List[ContextObjectDescription]]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "llm = OpenAILlamaIndex()\n",
    "\n",
    "chat_prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage.from_str(\n",
    "            \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. Here is the description:\\n {movie_name}\", role=\"user\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_description_llamaindex = llm.structured_predict(\n",
    "    InitialDescription, chat_prompt_tmpl, movie_name=\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\n",
    ")\n",
    "\n",
    "print(structured_description_llamaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91456c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object_description {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None} {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None}\n",
      "context_description {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': None, 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None} {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': 'table', 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None}\n",
      "room_description {'room_type': None, 'size': 'medium sized', 'additional_information': ['It appears to be a kitchen or living room.', 'The table is far from the wall.']} {'room_type': 'Kitchen', 'size': 'medium sized', 'additional_information': None}\n",
      "additional_information None None\n"
     ]
    }
   ],
   "source": [
    "# Compare structured_description_llamaindex and structured_description_struct_output by printing the values side by side\n",
    "for openai_key, llama_key in zip (structured_description_struct_output.model_dump().items(), structured_description_llamaindex.dict().items()):\n",
    "    print(openai_key[0], openai_key[1], llama_key[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6827b3",
   "metadata": {},
   "source": [
    "## AI2Thor wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a715eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "import ai2thor\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "42fa9180-6728-4c7b-a7cd-b5b11b991c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "from PIL import Image\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from descriptions import InitialDescription, ViewDescription\n",
    "from leolani_client import Action\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "from thor_utils import ( \n",
    "                        encode_image, \n",
    "                        get_distance,\n",
    "                        closest_objects\n",
    "                       )\n",
    "\n",
    "# Constants\n",
    "VISIBILITY_DISTANCE = 1.5\n",
    "SCENE = \"FloorPlan212\"\n",
    "\n",
    "class AI2ThorClient: \n",
    "    \"\"\"\n",
    "    An AI2Thor instance with methods wrapping its controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, leolaniClient, chat_mode):\n",
    "        self._controller = Controller(\n",
    "            agentMode=\"default\",\n",
    "            visibilityDistance=VISIBILITY_DISTANCE,\n",
    "            scene=SCENE,\n",
    "\n",
    "            # step sizes\n",
    "            gridSize=0.25,\n",
    "            snapToGrid=True,\n",
    "            rotateStepDegrees=90,\n",
    "\n",
    "            # image modalities\n",
    "            renderDepthImage=False,\n",
    "            renderInstanceSegmentation=False,\n",
    "\n",
    "            # camera properties\n",
    "            width=512,\n",
    "            height=512,\n",
    "            fieldOfView=90\n",
    "            )\n",
    "        self._metadata = []\n",
    "        self.descriptions = []\n",
    "        self.unstructured_descriptions = []\n",
    "        self.leolaniClient = leolaniClient\n",
    "        self._llm_ollama = OllamaLlamaIndex(model=\"llama3.2\", request_timeout=120.0)\n",
    "        self._llm_openai = OpenAILlamaIndex(model=\"gpt-4o-2024-08-06\")\n",
    "        self._llm_openai_multimodal = OpenAI()\n",
    "\n",
    "\n",
    "    def describe_view_from_image(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A string describing the current view.\n",
    "        \"\"\"\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "\n",
    "        response = self._llm_openai_multimodal.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective so start your message with 'I'. First, describe the objects, their colors, and their positions. Don't introduce your description. Start describing directly e.g. 'I currently see a <object> on a <surface> ...'. Be objective in your description! Finally describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else. If you can't infer the room type, just say so.\",\n",
    "                        },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.content)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    def describe_view_from_image_structured(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model with structure.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        ViewDescription\n",
    "            A structured description of the current view.\n",
    "        \"\"\"    \n",
    "\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "        \n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective. Describe the objects, their colors, and their positions. Be objective in your description! Describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else.\",\n",
    "                            },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            response_format=ViewDescription,\n",
    "            )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.parsed)\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "    def infer_room_type(self, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Infers the room type the agent is in.\n",
    "\n",
    "        Inference is based on:\n",
    "        - The image-to-text description of the view.\n",
    "        - The objects in the metadata.\n",
    "        - The AI2Thor object types mapping (https://ai2thor.allenai.org/ithor/documentation/objects/object-types).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns a string representing the likely room type.\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n",
    "    def parse_unstructured_description(self, description: str):\n",
    "        \"\"\"\n",
    "        Parse an unstructured description into structured data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        description : str\n",
    "            The unstructured description to parse.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        PydanticModel\n",
    "            An instance of the given Pydantic model populated with the parsed data.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"Your task is to turn a user's description of an object, its context and the room type into a structured response. \n",
    "                 When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. \n",
    "                 Only deviate from this rule when positions of objects in context are obvious, such as a floor (which is always below the target object) and a ceiling (which is above).\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ],\n",
    "            response_format=InitialDescription,\n",
    "        )\n",
    "\n",
    "        self.structured_initial_description = response.choices[0].message.parsed\n",
    "\n",
    "    def _get_image(self):\n",
    "        image = Image.fromarray(self._controller.last_event.frame)\n",
    "        # self.leolaniClient._add_image()\n",
    "        return image\n",
    "    \n",
    "        \n",
    "\n",
    "    def _step(self, direction: str = \"MoveAhead\", magnitude: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot takes one step in given direction. Options are:\n",
    "            - MoveAhead\n",
    "            - MoveBack\n",
    "            - MoveLeft\n",
    "            - MoveRight\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            moveMagnitude=magnitude\n",
    "            ) \n",
    "\n",
    "        action_attribute = getattr(Action, direction, None)\n",
    "        if action_attribute is not None:\n",
    "            self.leolaniClient._add_action(action_attribute)\n",
    "        else:\n",
    "            raise AttributeError(f\"'Action' object has no attribute '{direction}'\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _look(self, direction: str = \"LookUp\") -> None:\n",
    "        \"\"\"\n",
    "        Robot looks up or down. Options are:\n",
    "        - LookUp\n",
    "        - LookDown\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=30\n",
    "            )\n",
    "\n",
    "        self.leolaniClient._add_action(Action.direction)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _rotate(self, direction: str, degrees: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot turns in given direction (for optional degrees).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        direction : str\n",
    "            Direction to turn in. Can be \"RotateLeft\" or \"RotateRight\".\n",
    "        degrees : float, optional\n",
    "            Degrees to turn. Default is None.\n",
    "        \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=degrees\n",
    "            )\n",
    "        \n",
    "        if direction == \"RotateLeft\":\n",
    "            self.leolaniClient._add_action(Action.RotateLeft)\n",
    "        elif direction == \"RotateRight\":\n",
    "            self.leolaniClient._add_action(Action.RotateRight)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _crouch(self):\n",
    "        \"\"\"\n",
    "        Robot crouches.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Crouch\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Crouch)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _stand(self):\n",
    "        \"\"\"\n",
    "        Robot stands.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Stand\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Stand)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _teleport(self, position: dict = None, rotation: dict = None, horizon: float = None, standing: bool = None, to_random: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Robot teleports to random location.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        position: dict\n",
    "            The 'x', 'y', 'z' coordinates.\n",
    "        rotation: num\n",
    "            The rotation of the agent's body. If unspecified, the rotation of the agent remains the same.\n",
    "        horizon: Float\n",
    "            Look up of down. Negative values (e.g. -30) correspond to agent looking up, and vice versa.\n",
    "        standing: bool\n",
    "            True for \n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "\n",
    "        if to_random:\n",
    "            rotation = dict(x=0, y=random.randint(0, 360), z=0)\n",
    "            reachable_positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "            position = random.choice(reachable_positions)\n",
    "        \n",
    "        params = {\"action\": \"Teleport\", \"position\": position}\n",
    "        if rotation is not None:\n",
    "            params[\"rotation\"] = rotation\n",
    "        if horizon is not None:\n",
    "            params[\"horizon\"] = horizon\n",
    "        if standing is not None:\n",
    "            params[\"standing\"] = standing\n",
    "            \n",
    "        self._controller.step(**params)\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Teleport)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def _find_objects_in_sight(self, object_type: str = None) -> list:\n",
    "        \"\"\"\n",
    "        Finds objects in sight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object_type : str\n",
    "            The type of object to find.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of objects in sight.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get objects in sight\n",
    "        objects_in_sight = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"visible\"] == True]\n",
    "\n",
    "        # Optionally filter by object type\n",
    "        if object_type:\n",
    "            objects_in_sight = [obj for obj in objects_in_sight if obj[\"objectType\"] == object_type]\n",
    "\n",
    "        return objects_in_sight\n",
    "    \n",
    "    def _find_all_rooms(self, number=None):\n",
    "        \"\"\"\n",
    "        Create a list of all rooms (based on `roomType` == \"Floor\") in current scene. \n",
    "        Sorted from nearest to furthest.\n",
    "        \n",
    "        \"\"\"\n",
    "        rooms = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"objectType\"] == \"Floor\"]\n",
    "        rooms.sort(key=lambda room: room['distance'])\n",
    "        return rooms\n",
    "        \n",
    "    def _find_nearest_center_of_room(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary with \"x\", \"y\", \"z\" coordinates of nearest center of room(s).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        \"\"\"\n",
    "        rooms = self._find_all_rooms()\n",
    "        nearest_room = rooms[0]\n",
    "        center = nearest_room['axisAlignedBoundingBox']['center']\n",
    "        return center\n",
    "\n",
    "    def _done(self) -> None:\n",
    "        \"\"\"\n",
    "        The Done action does nothing to the state of the environment. \n",
    "        But, it returns a cleaned up event with respect to the metadata.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Done\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "494431fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMISSOR_PATH = \"./emissor\"\n",
    "AGENT = \"Human\"\n",
    "HUMAN = \"AI2ThorCLient\"\n",
    "from leolani_client import LeolaniChatClient, Action\n",
    "thor = AI2ThorClient(leolaniClient=LeolaniChatClient(emissor_path=EMISSOR_PATH, agent=AGENT, human=HUMAN), chat_mode=\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b5297c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor._teleport(position={\"x\":0, \"y\": 0.9009991884231567, \"z\": 0})\n",
    "thor._teleport(position={\"x\":1, \"y\": 0.9009991884231567, \"z\": 1})\n",
    "thor._teleport(position={\"x\":2, \"y\": 0.9009991884231567, \"z\": 2})\n",
    "thor._teleport(position={\"x\":3, \"y\": 0.9009991884231567, \"z\": 3})\n",
    "thor._teleport(to_random=True)\n",
    "thor._teleport(position=thor._find_nearest_center_of_room())\n",
    "# thor._rotate(direction=\"RotateRight\")\n",
    "# thor._step(direction=\"MoveAhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0c6c56a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently see a brown cardboard box on a reddish-brown wooden surface, which appears to be a table or cabinet. There is a shiny, metallic object close to the box. To my right, there is a light gray wall with a white door that has a gold handle. Beyond this door, there is another room with a dark brown wall and a white door at the end. The floor is light wood, and the lighting is soft, creating a calm atmosphere. The space suggests it might be a living room.\n"
     ]
    }
   ],
   "source": [
    "thor._find_objects_in_sight()\n",
    "print(thor.describe_view_from_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d4f9aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor._rotate(direction=\"RotateLeft\")\n",
    "thor._stand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a38d81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -4.0, 'y': 0.9009991884231567, 'z': -1.0}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -2.0, 'y': 0.9009991884231567, 'z': -0.5}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.5, 'y': 0.9009991884231567, 'z': 0.25}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for metadata in thor._metadata:\n",
    "    print(metadata['lastAction'])\n",
    "    print(metadata['agent']['position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43fa3543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I currently see a dark blue couch positioned against a light-colored wall. There is a plant peeking into view from the corner of the image. On the wall directly in front of me, there's a colorful abstract painting. The floor is made of wooden planks, giving a warm, inviting appearance. In the background, I notice a white door and what seems to be a hallway or another room. The overall arrangement and items present suggest that this is a living room.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor.describe_view_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dd651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Floor_e3656fd3', 'position': {'x': 0.0, 'y': 0.0, 'z': 0.0}, 'rotation': {'x': -0.0, 'y': 0.0, 'z': 0.0}, 'visible': True, 'isInteractable': True, 'receptacle': True, 'toggleable': False, 'isToggled': False, 'breakable': False, 'isBroken': False, 'canFillWithLiquid': False, 'isFilledWithLiquid': False, 'fillLiquid': None, 'dirtyable': False, 'isDirty': False, 'canBeUsedUp': False, 'isUsedUp': False, 'cookable': False, 'isCooked': False, 'temperature': 'Cold', 'isHeatSource': False, 'isColdSource': False, 'sliceable': False, 'isSliced': False, 'openable': False, 'isOpen': False, 'openness': 0.0, 'pickupable': False, 'isPickedUp': False, 'moveable': False, 'mass': 0.0, 'salientMaterials': None, 'receptacleObjectIds': ['Box|-02.00|+00.24|-02.66', 'SideTable|-05.73|+00.00|-01.28', 'Bed|-04.98|+00.00|-02.36', 'BaseballBat|-05.76|+00.14|-00.41', 'Doorway|-02.24|-00.01|+00.04', 'Desk|-05.52|00.00|-00.30', 'Chair|-04.43|00.00|-00.34', 'Shelf|-05.52|+00.10|-00.30', 'BasketBall|-02.46|+00.12|-02.28', 'Plunger|-01.88|00.00|-02.12', 'ScrubBrush|-01.67|+00.00|-02.17', 'ShowerCurtain|-01.25|+01.94|-02.49', 'Toilet|-00.35|+00.00|-01.89', 'Doorway|-00.88|+00.00|+00.03', 'CounterTop|-00.13|+00.00|-00.71', 'DogBed|-05.80|+00.00|+02.74', 'Drawer|-05.52|+00.23|+01.95', 'CoffeeTable|-05.52|+00.00|+01.76', 'Drawer|-05.52|+00.07|+01.95', 'Drawer|-05.52|+00.07|+01.57', 'Drawer|-05.52|+00.23|+01.57', 'ArmChair|-03.91|+00.00|+00.82', 'Doorway|-03.25|-00.01|+01.00', 'SideTable|-04.79|+00.00|+00.63', 'Shelf|-04.79|+00.26|+00.63', 'FloorLamp|+04.65|+00.00|+02.61', 'SideTable|+03.70|+00.00|+02.64', 'Shelf|+03.70|+00.26|+02.64', 'TVStand|+02.56|+00.00|+02.69', 'Shelf|+02.56|+00.24|+02.69', 'CoffeeTable|+03.57|+00.00|-00.12', 'Sofa|+02.42|+00.00|-00.11', 'Doorway|+06.24|+00.00|-00.27', 'Stool|+05.76|+00.00|-00.93', 'Stool|+05.22|+00.00|-01.58', 'DiningTable|+05.76|+00.00|-01.69', 'Stool|+05.70|+00.00|-02.66', 'HousePlant|+03.62|+00.13|-00.10', 'GarbageCan|+03.65|00.00|-02.92', 'CounterTop|+02.40|+00.00|-01.02', 'CounterTop|+02.53|+00.00|-02.81', 'Fridge|+01.00|+00.00|-02.79'], 'distance': 3.8889973163604736, 'objectType': 'Floor', 'objectId': 'Floor|+00.00|+00.00|+00.00', 'assetId': '', 'parentReceptacles': None, 'controlledObjects': None, 'isMoving': False, 'axisAlignedBoundingBox': {'cornerPoints': [[6.2437944412231445, 0.0, 3.2035932540893555], [6.2437944412231445, 0.0, -3.2452797889709473], [6.2437944412231445, -0.3243083953857422, 3.2035932540893555], [6.2437944412231445, -0.3243083953857422, -3.2452797889709473], [-6.2437944412231445, 0.0, 3.2035932540893555], [-6.2437944412231445, 0.0, -3.2452797889709473], [-6.2437944412231445, -0.3243083953857422, 3.2035932540893555], [-6.2437944412231445, -0.3243083953857422, -3.2452797889709473]], 'center': {'x': 0.0, 'y': -0.1621541976928711, 'z': -0.0208432674407959}, 'size': {'x': 12.487588882446289, 'y': 0.3243083953857422, 'z': 6.448873043060303}}, 'objectOrientedBoundingBox': None}]\n"
     ]
    }
   ],
   "source": [
    "rooms = [obj for obj in thor._metadata[-1][\"objects\"] if obj[\"objectType\"] == \"Floor\"]\n",
    "rooms.sort(key=lambda room: room['distance'])\n",
    "\n",
    "print(rooms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97a44cc974298805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0.0, 'y': -0.1621541976928711, 'z': -0.0208432674407959}\n"
     ]
    }
   ],
   "source": [
    "print(rooms[0]['axisAlignedBoundingBox']['center'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c0e15ce3df295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901a15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArmChair|-00.27|+00.00|+01.87\n",
      "ArmChair|+02.66|+00.00|+01.86\n",
      "Boots|+04.00|+00.00|+01.70\n",
      "Box|-00.47|+01.04|-00.71\n",
      "CoffeeTable|+01.59|00.00|+00.45\n",
      "CreditCard|+01.41|+00.47|+00.65\n",
      "Drawer|+03.88|+00.77|+00.86\n",
      "Floor|+00.00|+00.00|+00.00\n",
      "FloorLamp|+03.61|+00.00|+02.16\n",
      "GarbageCan|+03.83|-00.03|-00.50\n",
      "HousePlant|+00.39|+00.80|-00.73\n",
      "KeyChain|+01.50|+00.47|+00.53\n",
      "Laptop|+01.80|+00.47|+00.50\n",
      "LightSwitch|-01.40|+01.29|+01.84\n",
      "Newspaper|+02.15|+00.41|-00.72\n",
      "Painting|+04.07|+01.95|+00.85\n",
      "Pen|+03.93|+00.87|+01.04\n",
      "Pencil|+03.89|+00.87|+01.18\n",
      "Pillow|+00.65|+00.39|+01.71\n",
      "RemoteControl|+01.88|+00.33|+01.73\n",
      "Shelf|-00.29|+00.59|-00.73\n",
      "Shelf|+01.91|+00.20|-00.73\n",
      "Shelf|-00.29|+00.20|-00.73\n",
      "Shelf|+01.91|+00.59|-00.73\n",
      "SideTable|+03.95|+00.00|+00.86\n",
      "Sofa|+01.19|+00.01|+01.87\n",
      "Statue|-00.09|+00.03|-00.70\n",
      "Statue|-00.54|+00.40|-00.69\n",
      "Television|+01.90|+01.28|-00.84\n",
      "TissueBox|+03.92|+00.87|+00.68\n",
      "TVStand|-00.29|00.00|-00.77\n",
      "TVStand|+01.90|00.00|-00.77\n",
      "WateringCan|+01.62|+00.02|-00.70\n",
      "Window|+01.57|+02.07|+02.49\n",
      "Window|+00.02|+02.07|+02.49\n"
     ]
    }
   ],
   "source": [
    "thor.teleport(to_random=True)\n",
    "last_event = thor._events[-1]\n",
    "objects = last_event[\"objects\"]\n",
    "\n",
    "for obj in objects:\n",
    "    print(obj[\"objectId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb43c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\", additional_kwargs={'tool_calls': []}, raw={'model': 'llama3.2', 'created_at': '2024-11-15T09:08:54.133358Z', 'message': {'role': 'assistant', 'content': \"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 5177552708, 'load_duration': 3339315041, 'prompt_eval_count': 30, 'prompt_eval_duration': 79901000, 'eval_count': 91, 'eval_duration': 1756324000, 'usage': {'prompt_tokens': 30, 'completion_tokens': 91, 'total_tokens': 121}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor._llm_ollama.complete(\"What do you see?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb1090",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1dca3b-b338-4146-b079-0586c0653dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found the object!\n"
     ]
    }
   ],
   "source": [
    "class InitialDescriptionComplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class InitialDescriptionIncomplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectFound(Event):\n",
    "    payload: str\n",
    "\n",
    "class WrongObjectSuggested(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomCorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomIncorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectNotInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "number = 1\n",
    "\n",
    "class ThorFindsObject(Workflow):\n",
    "    \n",
    "    def __init__(self, timeout: int = 10, verbose: bool = False):\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "        self.thor = AI2ThorClient()\n",
    "    \n",
    "    @step\n",
    "    async def evaluate_initial_description(self, ev: StartEvent) -> InitialDescriptionComplete | InitialDescriptionIncomplete:\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return InitialDescriptionComplete(payload=\"Initial description is complete.\")\n",
    "        else:\n",
    "            return InitialDescriptionIncomplete(payload=\"Initial description is incomplete.\")\n",
    "\n",
    "    @step\n",
    "    async def clarify_initial_description(self, ev: InitialDescriptionIncomplete) -> InitialDescriptionComplete:\n",
    "        return InitialDescriptionComplete(payload=\"Description clarified.\")\n",
    "\n",
    "    @step\n",
    "    async def find_correct_room_type(self, ev: InitialDescriptionComplete | RoomIncorrect | ObjectNotInRoom) -> RoomCorrect | RoomIncorrect:\n",
    "        current_description = \"\"\"a living room setup viewed from behind a dark-colored couch. The room has light-colored walls and a floor that seems to be a muted, earthy tone. The main items in the room include:\n",
    "- A large, dark-colored sofa in the foreground facing a TV.\n",
    "- A television placed on a small white TV stand, positioned along the far wall.\n",
    "- A small side table with a blue vase and a decorative item beside the TV stand.\n",
    "- A wooden shelf or cabinet off to the left side of the room.\"\"\"\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return RoomCorrect(payload=\"Correct room is found.\")\n",
    "        else:\n",
    "            return RoomIncorrect(payload=\"Correct room is not found.\")\n",
    "\n",
    "    @step \n",
    "    async def find_object_in_room(self, ev: RoomCorrect) -> ObjectInRoom | ObjectNotInRoom:\n",
    "        if random.randint(0, 10) < 4:\n",
    "            return ObjectInRoom(payload=\"Object may be in this room.\")\n",
    "        else:\n",
    "            return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "    \n",
    "    @step \n",
    "    async def suggest_object(self, ev: ObjectInRoom | WrongObjectSuggested) -> WrongObjectSuggested | ObjectNotInRoom | StopEvent:\n",
    "        \n",
    "        \n",
    "        if  random.randint(0, 1) == 0:\n",
    "            return StopEvent(result=\"We found the object!\")  # End the workflow\n",
    "        else:\n",
    "            if random.randint(0, 1) == 0:\n",
    "                return WrongObjectSuggested(payload=\"Couldn't find object in this room.\")\n",
    "            else:\n",
    "                return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Initialize and run the workflow\n",
    "w = ThorFindsObject(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81d8f70-1c60-4eba-87b3-daf0233f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionIncomplete'>\n",
      "<class '__main__.RoomCorrect'>\n",
      "<class '__main__.RoomIncorrect'>\n",
      "<class '__main__.ObjectInRoom'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class '__main__.WrongObjectSuggested'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "possible_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(ThorFindsObject, filename=\"possible_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbad389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chainlit in ./myenv/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2.1)\n",
      "Requirement already satisfied: asyncer<0.0.8,>=0.0.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.7)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: dataclasses_json<0.7.0,>=0.6.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.6.7)\n",
      "Requirement already satisfied: fastapi<0.116,>=0.115.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.115.4)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.27.2)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: literalai==0.0.623 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.623)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.0.1)\n",
      "Requirement already satisfied: python-multipart<0.0.10,>=0.0.9 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.9)\n",
      "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (5.11.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.41.2 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.41.2)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.2)\n",
      "Requirement already satisfied: uptrace<2.0.0,>=1.22.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.27.0)\n",
      "Requirement already satisfied: uvicorn<0.26.0,>=0.25.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.25.0)\n",
      "Requirement already satisfied: watchfiles<0.21.0,>=0.20.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.20.0)\n",
      "Requirement already satisfied: chevron>=0.14.0 in ./myenv/lib/python3.11/site-packages (from literalai==0.0.623->chainlit) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in ./myenv/lib/python3.11/site-packages (from asyncer<0.0.8,>=0.0.7->chainlit) (4.6.2.post1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.11/site-packages (from fastapi<0.116,>=0.115.3->chainlit) (4.12.2)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.0.6)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (3.10)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (2.23.4)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.10.1)\n",
      "Requirement already satisfied: opentelemetry-api~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation~=0.48b0 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: requests~=2.7 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-proto==1.28.0->opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.16.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in ./myenv/lib/python3.11/site-packages (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./myenv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.7.0,>=0.6.7->chainlit) (1.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.2)\n",
      "Requirement already satisfied: wsproto in ./myenv/lib/python3.11/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3cf313-de13-45b7-b15b-2902740e1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "from PIL import Image\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from descriptions import InitialDescription, ViewDescription\n",
    "from leolani_client import Action\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import base64\n",
    "import torchvision\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import json\n",
    "import time\n",
    "from thor_utils import ( \n",
    "                        encode_image, \n",
    "                        get_distance,\n",
    "                        closest_objects,\n",
    "                        map_detected_to_visible_objects,\n",
    "                        select_objects,\n",
    "                        calculate_turn_angle,\n",
    "                        expand_box,\n",
    "                        calculate_turn_angle,\n",
    "                        compute_final_angle,\n",
    "                        calculate_turn_and_distance_dot_product\n",
    "\t\t\t)\n",
    "\n",
    "# Constants\n",
    "VISIBILITY_DISTANCE = 15\n",
    "SCENE = \"FloorPlan209\"\n",
    "\n",
    "class AI2ThorClient: \n",
    "    \"\"\"\n",
    "    An AI2Thor instance with methods wrapping its controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, leolaniClient, chat_mode: str = \"Production\", workflow = None):\n",
    "        self._controller = Controller(\n",
    "            agentMode=\"default\",\n",
    "            visibilityDistance=VISIBILITY_DISTANCE,\n",
    "            scene=SCENE,\n",
    "\n",
    "            # step sizes\n",
    "            gridSize=0.25,\n",
    "            snapToGrid=True,\n",
    "            rotateStepDegrees=90,\n",
    "\n",
    "            # image modalities\n",
    "            renderDepthImage=False,\n",
    "            renderInstanceSegmentation=False,\n",
    "\n",
    "            # camera properties\n",
    "            width=512,\n",
    "            height=512,\n",
    "            fieldOfView=90\n",
    "            )\n",
    "\n",
    "        self._metadata = []\n",
    "        self.descriptions = []\n",
    "        self.unstructured_descriptions = []\n",
    "        self.leolaniClient = leolaniClient\n",
    "        self._llm_ollama = OllamaLlamaIndex(model=\"llama3.2\", request_timeout=120.0)\n",
    "        self._llm_openai = OpenAILlamaIndex(model=\"gpt-4o-2024-08-06\", api_key=api_key )\n",
    "        self._llm_openai_multimodal = OpenAI(api_key=api_key )\n",
    "        self._chat_mode = chat_mode\n",
    "        self._workflow = workflow\n",
    "        self.objects_seen = {}\n",
    "        self._clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval()\n",
    "        self._frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()\n",
    "        self._clip_processor = clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self._similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self._rooms = self._find_all_rooms()\n",
    "        self._rooms_visited = []\n",
    "        self._objects_seen = {}\n",
    "        self._clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval()\n",
    "        self._frcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).eval()\n",
    "        self._clip_processor = clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self._similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self._rooms = self._find_all_rooms()\n",
    "        self._rooms_visited = []\n",
    "\n",
    "    def describe_view_from_image(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A string describing the current view.\n",
    "        \"\"\"\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "\n",
    "        response = self._llm_openai_multimodal.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective so start your message with 'I'. First, describe the objects, their colors, and their positions. Don't introduce your description. Start describing directly e.g. 'I currently see a <object> on a <surface> ...'. Be objective in your description! Finally describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else. If you can't infer the room type, just say so.\",\n",
    "                        },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.content)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    def describe_view_from_image_structured(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model with structure.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        ViewDescription\n",
    "            A structured description of the current view.\n",
    "        \"\"\"    \n",
    "\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "        \n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective. Describe the objects, their colors, and their positions. Be objective in your description! Describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else.\",\n",
    "                            },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            response_format=ViewDescription,\n",
    "            )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.parsed)\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "    def infer_room_type(self, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Infers the room type the agent is in.\n",
    "\n",
    "        Inference is based on:\n",
    "        - The image-to-text description of the view.\n",
    "        - The objects in the metadata.\n",
    "        - The AI2Thor object types mapping (https://ai2thor.allenai.org/ithor/documentation/objects/object-types).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns a string representing the likely room type.\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n",
    "    def parse_unstructured_description(self, description: str):\n",
    "        \"\"\"\n",
    "        Parse an unstructured description into structured data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        description : str\n",
    "            The unstructured description to parse.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        PydanticModel\n",
    "            An instance of the given Pydantic model populated with the parsed data.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"Your task is to turn a user's description of an object, its context and the room type into a structured response. \n",
    "                 When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. \n",
    "                 Only deviate from this rule when positions of objects in context are obvious, such as a floor (which is always below the target object) and a ceiling (which is above).\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ],\n",
    "            response_format=InitialDescription,\n",
    "        )\n",
    "\n",
    "        self.structured_initial_description = response.choices[0].message.parsed\n",
    "\n",
    "    def _get_image(self):\n",
    "        image = Image.fromarray(self._controller.last_event.frame)\n",
    "        \n",
    "        # self.leolaniClient._add_image()\n",
    "        if self._chat_mode == \"Developer\":\n",
    "            self._workflow.send_message(content=image)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "        \n",
    "\n",
    "    def _step(self, direction: str = \"MoveAhead\", magnitude: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot takes one step in given direction. Options are:\n",
    "            - MoveAhead\n",
    "            - MoveBack\n",
    "            - MoveLeft\n",
    "            - MoveRight\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            moveMagnitude=magnitude\n",
    "            ) \n",
    "\n",
    "        action_attribute = getattr(Action, direction, None)\n",
    "        if action_attribute is not None:\n",
    "            self.leolaniClient._add_action(action_attribute)\n",
    "        else:\n",
    "            raise AttributeError(f\"'Action' object has no attribute '{direction}'\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _look(self, direction: str = \"LookUp\") -> None:\n",
    "        \"\"\"\n",
    "        Robot looks up or down. Options are:\n",
    "        - LookUp\n",
    "        - LookDown\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=30\n",
    "            )\n",
    "\n",
    "        self.leolaniClient._add_action(Action.direction)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _rotate(self, direction: str, degrees: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot turns in given direction (for optional degrees).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        direction : str\n",
    "            Direction to turn in. Can be \"RotateLeft\" or \"RotateRight\".\n",
    "        degrees : float, optional\n",
    "            Degrees to turn. Default is None.\n",
    "        \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=degrees\n",
    "            )\n",
    "        \n",
    "        if direction == \"RotateLeft\":\n",
    "            self.leolaniClient._add_action(Action.RotateLeft)\n",
    "        elif direction == \"RotateRight\":\n",
    "            self.leolaniClient._add_action(Action.RotateRight)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _crouch(self):\n",
    "        \"\"\"\n",
    "        Robot crouches.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Crouch\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Crouch)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _stand(self):\n",
    "        \"\"\"\n",
    "        Robot stands.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Stand\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Stand)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _teleport(self, position: dict = None, rotation: dict = None, horizon: float = None, standing: bool = None, to_random: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Robot teleports to random location.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        position: dict\n",
    "            The 'x', 'y', 'z' coordinates.\n",
    "        rotation: num\n",
    "            The rotation of the agent's body. If unspecified, the rotation of the agent remains the same.\n",
    "        horizon: Float\n",
    "            Look up of down. Negative values (e.g. -30) correspond to agent looking up, and vice versa.\n",
    "        standing: bool\n",
    "            True for \n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "\n",
    "        if to_random:\n",
    "            rotation = dict(x=0, y=random.randint(0, 360), z=0)\n",
    "            reachable_positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "            position = random.choice(reachable_positions)\n",
    "        \n",
    "        params = {\"action\": \"Teleport\", \"position\": position}\n",
    "        if rotation is not None:\n",
    "            params[\"rotation\"] = rotation\n",
    "        if horizon is not None:\n",
    "            params[\"horizon\"] = horizon\n",
    "        if standing is not None:\n",
    "            params[\"standing\"] = standing\n",
    "            \n",
    "        self._controller.step(**params)\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Teleport)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def _find_objects_in_sight(self, object_type: str = None) -> list:\n",
    "        \"\"\"\n",
    "        Finds objects in sight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object_type : str\n",
    "            The type of object to find.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of objects in sight.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get objects in sight\n",
    "        objects_in_sight = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"visible\"] == True]\n",
    "        objects_in_sight = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"visible\"] == True]\n",
    "\n",
    "        # Optionally filter by object type\n",
    "        if object_type:\n",
    "            objects_in_sight = [obj for obj in objects_in_sight if obj[\"objectType\"] == object_type]\n",
    "\n",
    "        for obj in objects_in_sight:\n",
    "\n",
    "            # Use a unique identifier for the object (e.g., object ID or position)\n",
    "            object_id = obj[\"objectId\"]  #  maybe no position\n",
    "    \n",
    "    \n",
    "    \n",
    "            # If the object is not already in the global dictionary, add it\n",
    "            if object_id not in self._objects_seen.keys():\n",
    "                # Add a Visited attribute with a default value of 0\n",
    "                obj[\"visited\"] = 0\n",
    "                self._objects_seen[object_id] = obj\n",
    "\n",
    "        return objects_in_sight\n",
    "    \n",
    "    def _find_all_rooms(self, number=None):\n",
    "        \"\"\"\n",
    "        Create a list of all rooms (based on `roomType` == \"Floor\") in current scene. \n",
    "        Sorted from nearest to furthest.\n",
    "        \n",
    "        \"\"\"\n",
    "        rooms = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"objectType\"] == \"Floor\"]\n",
    "        rooms.sort(key=lambda room: room['distance'])\n",
    "        return rooms\n",
    "    \n",
    "    def find_nearest_reachable_position(self, destination) -> dict:\n",
    "        \"\"\"\n",
    "        Find a reachable position that is nearest to the given destination.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        destination: dict\n",
    "            A dictionary of x, y, z coordinates of the destination.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict:\n",
    "            a dictionary of x, y, z coordinates representing the nearest reachable position.\n",
    "        \"\"\"\n",
    "        pass    \n",
    "\n",
    "    def _teleport_to_nearest_new_room(self) -> str:\n",
    "        \"\"\"\n",
    "        Teleports the agent to the center of the nearest room if reachable.\n",
    "        If not, teleports to the nearest reachable position to the center.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The `objectId` of the room teleported to.\n",
    "        \"\"\"\n",
    "        rooms = self._find_all_rooms()\n",
    "        \n",
    "        # Iterate over rooms to find nearest non-visited room\n",
    "        for room in rooms:\n",
    "            if room not in self._rooms_visited:\n",
    "                destination_room = room\n",
    "         \n",
    "        # Find the nearest room's center\n",
    "        center = destination_room['axisAlignedBoundingBox']['center']\n",
    "\n",
    "        # Get reachable positions\n",
    "        reachable_positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "        \n",
    "        # Check if the nearest room's center is reachable\n",
    "        if center in reachable_positions:\n",
    "            self._rooms_visited.append(destination_room)\n",
    "            return self._teleport(position=center)\n",
    "        else:\n",
    "            # Find the reachable position nearest to the room's center\n",
    "            nearest_reachable_position = find_nearest_reachable_position(center)\n",
    "            if nearest_reachable_position:\n",
    "                self._rooms_visited.append(destination_room)\n",
    "                return self._teleport(position=nearest_reachable_position)\n",
    "            else:\n",
    "                return self._teleport_to_nearest_new_room()\n",
    "\n",
    "    def _done(self) -> None:\n",
    "        \"\"\"\n",
    "        The Done action does nothing to the state of the environment. \n",
    "        But, it returns a cleaned up event with respect to the metadata.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Done\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "\n",
    "    def _find_objects_and_angles(self, image, target_objects):\n",
    "        \"\"\"Main workflow to find objects and compute turn angles.\"\"\"\n",
    "\n",
    "        # Detection\n",
    "        detections, image_width, image_height = self._detect_objects(image)\n",
    "\n",
    "        # Classification\n",
    "        detections_df = self._classify_objects(detections, image, target_objects)\n",
    "\n",
    "        # detections_df = pd.DataFrame(detections_with_labels)\n",
    "        if len(detections_df) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # Object Selection\n",
    "        object_role, selected_objects = select_objects(detections_df, target_objects)\n",
    "\n",
    "        # Angle Calculation\n",
    "        turn_angle = compute_final_angle(selected_objects, image_width)\n",
    "\n",
    "        return turn_angle, detections_df, selected_objects\n",
    "\n",
    "\n",
    "\n",
    "    def _attempt_to_find_and_go_to_target(self, target_label: str, context_objects: list, logs: list, agent_into :tuple) -> tuple:\n",
    "        \"\"\"\n",
    "        Attempt to locate the target object and move toward it.\n",
    "        If the agent cannot step, teleport closer to the target.\n",
    "\n",
    "        Args:\n",
    "            target_label (str): Name of the target object.\n",
    "            context_objects (list): List of described object names.\n",
    "            logs (list): A list to collect log messages.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the target object is reached, False otherwise.\n",
    "        \"\"\"\n",
    "        turn_angle=None\n",
    "        matched_object=None\n",
    "        count_of_turns = agent_info[0]\n",
    "        agent_rot=agent_info[2]\n",
    "        agent_pos=agent_info[1]\n",
    "\n",
    "        if agent_rot != None or agent_pos != None:\n",
    "\n",
    "            self._teleport(position=agent_pos, rotation=agent_rot, horizon=0)\n",
    "            \n",
    "\n",
    "        # Get initial agent position and rotation\n",
    "        agent_position = self._metadata[-1]['agent']['position']\n",
    "        agent_rotation = self._metadata[-1]['agent']['rotation']\n",
    "        logs.append(f'Started looking for the {target_label} in the new room!')\n",
    "        while (turn_angle is None or matched_object is None):\n",
    "            image = self._get_image()  # Update the image after each rotation\n",
    "            \n",
    "            # Define target and described objects\n",
    "            target_objects = {'target': target_label, 'context': context_objects}\n",
    "            # Find the object using metadata\n",
    "            meta_best_match, meta_turn_angle = self._find_target_angle_and_id_with_meta(target_objects['target'])\n",
    "\n",
    "            # Run the workflow to find objects and calculate turn angles\n",
    "            turn_angle, detections_df, objects_selected = self._find_objects_and_angles(\n",
    "                image=image,\n",
    "                target_objects=target_objects\n",
    "            )\n",
    "            visible_objects = self._find_objects_in_sight(object_type=None)\n",
    "\n",
    "            matched_object, logs = self._select_objects_by_similarity(logs, target_label, visible_objects)\n",
    "            if matched_object!= None and turn_angle != None:\n",
    "                logs.append(f'Found a possible object! Trying now to approach it.')\n",
    "\n",
    "            \n",
    "            if turn_angle is None and meta_turn_angle is not None:\n",
    "                turn_angle = meta_turn_angle\n",
    "                matched_object = meta_best_match\n",
    "                logs.append(f'Did not find the object through camera, but found one through the Metadata.')\n",
    "\n",
    "            \n",
    "            for item in self._objects_seen.values():  # Iterate over the dictionary values\n",
    "\n",
    "                if matched_object is not None and item['objectId'] == matched_object['id']:\n",
    "                    if item['visited'] == 1:\n",
    "                        turn_angle = None\n",
    "                        matched_object = None\n",
    "                        print('already seen the object')\n",
    "                        logs.append(f'The object that was found has already been found before. Continueing the search!')\n",
    "\n",
    "            if turn_angle is None and meta_turn_angle is None and matched_object != None:\n",
    "                for item in self._objects_seen.values():\n",
    "                    if item['objectId'] == matched_object['id']:\n",
    "                        item['visited'] = 1\n",
    "                logs.append('The navigation to the found object could not be calculated, but the object ID is known!')\n",
    "                return matched_object['id'], logs, (count_of_turns, agent_position, agent_rotation)\n",
    "                        \n",
    "            if not turn_angle or not matched_object:\n",
    "                self._rotate(direction='RotateLeft')  # Rotate to search for objects\n",
    "                agent_position = self._metadata[-1]['agent']['position']\n",
    "                agent_rotation = self._metadata[-1]['agent']['rotation']\n",
    "                logs.append(f\"Rotated left to search for '{target_label}'.\")\n",
    "                count_of_turns += 1\n",
    "\n",
    "            if count_of_turns >= 3:\n",
    "                logs.append(f\"Target '{target_label}' not found after 3 rotations.\")\n",
    "                return False, logs, (count_of_turns, agent_position, agent_rotation)\n",
    "    \n",
    "\n",
    "\n",
    "        logs.append(f\"Matched target '{target_label}' to object: {matched_object}.\")\n",
    "        selected_object = matched_object\n",
    "\n",
    "        # Rotate to align with the selected object\n",
    "        if turn_angle > 0:\n",
    "            self._rotate(direction='RotateRight', degrees=abs(turn_angle))\n",
    "            logs.append(f\"Rotated right by {abs(turn_angle)} degrees to align with the target.\")\n",
    "        else:\n",
    "            self._rotate(direction='RotateLeft', degrees=abs(turn_angle))\n",
    "            logs.append(f\"Rotated left by {abs(turn_angle)} degrees to align with the target.\")\n",
    "\n",
    "        # Step toward the target object\n",
    "        max_teleports = 10  # Prevent infinite retries\n",
    "        teleport_count = 0\n",
    "        step_count = 0\n",
    "        max_steps = 20\n",
    "\n",
    "        while teleport_count <= max_teleports:\n",
    "            while step_count < max_steps:\n",
    "                agent_position = self._metadata[-1]['agent']['position']\n",
    "                target_position = selected_object[\"position\"]\n",
    "\n",
    "                distance = get_distance(agent_position, target_position)\n",
    "                print(f'{distance}, distance')\n",
    "\n",
    "                if distance <= 1.5:\n",
    "                    logs.append(f\"Target '{target_label}' is within {distance:.2f} meters. Successfully reached.\")\n",
    "                    for item in self._objects_seen.values():\n",
    "                        if item['objectId'] == selected_object['id']:\n",
    "                            item['visited'] = 1\n",
    "                    return selected_object['id'], logs, (count_of_turns, agent_position, agent_rotation)\n",
    "\n",
    "                self._step('MoveAhead')\n",
    "                logs.append('Stepped forward.')\n",
    "                step_count += 1\n",
    "\n",
    "                if not self._metadata[-1]['lastActionSuccess']:\n",
    "                    logs.append(\"Step failed. Calculating closest teleportable position.\")\n",
    "                    break\n",
    "\n",
    "            teleport_position = self._calculate_closest_teleportable_position(agent_position, target_position)\n",
    "            if teleport_position is None:\n",
    "                logs.append(\"No suitable teleportable position found.\")\n",
    "                for item in self._objects_seen.values():\n",
    "                    if item['objectId'] == selected_object['id']:\n",
    "                        item['visited'] = 1\n",
    "                return selected_object['id'], logs, (count_of_turns, agent_position, agent_rotation)\n",
    "\n",
    "            self._teleport(position=teleport_position)\n",
    "            teleport_count += 1\n",
    "            logs.append(f\"Teleported to {teleport_position}. Resuming movement.\")\n",
    "\n",
    "        logs.append(\"Max teleports reached. Could not reach the target.\")\n",
    "        for item in self._objects_seen.values():\n",
    "            if item['objectId'] == selected_object['id']:\n",
    "                item['visited'] = 1\n",
    "        return selected_object['id'], logs, (count_of_turns, agent_position, agent_rotation)\n",
    "\n",
    "\n",
    "\n",
    "    def _select_objects_by_similarity(self, logs, target_label: str, visible_objects: list, similarity_threshold=0.40) -> list:\n",
    "        \"\"\"\n",
    "        Select the best matching visible object for the target label based on semantic similarity using embeddings.\n",
    "    \n",
    "        Args:\n",
    "            logs (list): Log messages.\n",
    "            target_label (str): The target label to match.\n",
    "            visible_objects (list): List of visible objects, each as a dictionary with 'objectType' and other properties.\n",
    "            similarity_threshold (float): The minimum cosine similarity required to consider a match.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: (Best match object or None, Updated logs)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Extract object types and metadata from visible objects\n",
    "            object_data = [\n",
    "                {\"id\": obj[\"objectId\"], \"type\": obj[\"objectType\"], \"position\": obj[\"position\"]}\n",
    "                for obj in visible_objects\n",
    "            ]\n",
    "            object_types = [obj[\"type\"] for obj in object_data]\n",
    "    \n",
    "            if not object_types:\n",
    "                logs.append(\"No object types found in visible objects.\")\n",
    "                return None, logs\n",
    "    \n",
    "            # Step 2: Encode target label and object types\n",
    "            target_embedding = self._similarity_model.encode([target_label])\n",
    "            object_embeddings = self._similarity_model.encode(object_types)\n",
    "    \n",
    "            if target_embedding.size == 0 or len(object_embeddings) == 0:\n",
    "                logs.append(\"Embeddings for target or objects are missing or invalid.\")\n",
    "                return None, logs\n",
    "    \n",
    "            # Step 3: Compute cosine similarity between target label and object types\n",
    "            similarities = cosine_similarity(target_embedding, object_embeddings).flatten()\n",
    "    \n",
    "            # Step 4: Find the best match based on similarity\n",
    "            best_match_index = np.argmax(similarities)\n",
    "            best_similarity = similarities[best_match_index]\n",
    "    \n",
    "            if best_similarity < similarity_threshold:\n",
    "                logs.append(f\"No object matches the target label '{target_label}' with sufficient similarity (Threshold: {similarity_threshold}).\")\n",
    "                return None, logs\n",
    "    \n",
    "            # Step 5: Return the best match object metadata\n",
    "            best_match_object = object_data[best_match_index]\n",
    "            logs.append(f\"Best match for '{target_label}' is '{best_match_object['type']}' with similarity {best_similarity:.2f}.\")\n",
    "            return best_match_object, logs\n",
    "    \n",
    "        except Exception as e:\n",
    "            logs.append(f\"Error during object similarity computation: {e}\")\n",
    "            return None, logs\n",
    "\n",
    "    \n",
    "    def _calculate_relative_position(self, agent_position: dict, agent_rotation: dict, object_position: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the relative position of an object with respect to the agent's direction.\n",
    "    \n",
    "        Args:\n",
    "            agent_position (dict): The agent's current position (x, y, z).\n",
    "            agent_rotation (dict): The agent's current rotation (yaw, pitch, roll).\n",
    "            object_position (dict): The object's position (x, y, z).\n",
    "    \n",
    "        Returns:\n",
    "            dict: A dictionary with relative position information:\n",
    "                  - distance (float): The Euclidean distance between the agent and the object.\n",
    "                  - angle (float): The relative angle of the object to the agent's forward direction.\n",
    "        \"\"\"\n",
    "        import math\n",
    "    \n",
    "        # Calculate the vector from the agent to the object\n",
    "        dx = object_position[\"x\"] - agent_position[\"x\"]\n",
    "        dz = object_position[\"z\"] - agent_position[\"z\"]\n",
    "    \n",
    "        # Calculate the Euclidean distance\n",
    "        distance = (dx**2 + dz**2) ** 0.5\n",
    "    \n",
    "        # Calculate the angle relative to the agent's forward direction\n",
    "        \n",
    "        agent_yaw = agent_rotation[\"yaw\"]  # Agent's forward direction (in degrees)\n",
    "        object_angle = math.degrees(math.atan2(dz, dx))  # Object's angle relative to the origin\n",
    "        relative_angle = (object_angle - agent_yaw + 360) % 360  # Normalize to [0, 360)\n",
    "    \n",
    "        # Normalize angle to [-180, 180] for easier directional interpretation\n",
    "        if relative_angle > 180:\n",
    "            relative_angle -= 360\n",
    "    \n",
    "        return {\"distance\": distance, \"angle\": relative_angle}\n",
    "    \n",
    "        \n",
    "    def _calculate_closest_teleportable_position(self, agent_position: dict, target_position: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the closest teleportable position along the line between the agent and the target.\n",
    "    \n",
    "        Args:\n",
    "            agent_position (dict): The agent's current position.\n",
    "            target_position (dict): The target's position.\n",
    "    \n",
    "        Returns:\n",
    "            dict or None: The closest teleportable position if found, otherwise None.\n",
    "        \"\"\"\n",
    "        # Extract teleportable positions from metadata\n",
    "        teleportable_positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "    \n",
    "        if not teleportable_positions:\n",
    "            raise ValueError(\"No teleportable positions found in the event metadata.\")\n",
    "    \n",
    "        closest_position = None\n",
    "        min_distance = float('inf')\n",
    "    \n",
    "        for position in teleportable_positions:\n",
    "            # Calculate Euclidean distance between the agent and each teleportable position\n",
    "            distance = math.sqrt(\n",
    "                (position[\"x\"] - target_position[\"x\"]) ** 2 +\n",
    "                (position[\"z\"] - target_position[\"z\"]) ** 2\n",
    "            )\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_position = position\n",
    "    \n",
    "        return closest_position\n",
    "\n",
    "    \n",
    "    def _is_on_line(self, agent_position: dict, target_position: dict, point: dict) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if a point is on the line segment between the agent and the target.\n",
    "    \n",
    "        Args:\n",
    "            agent_position (dict): Current position of the agent.\n",
    "            target_position (dict): Position of the target object.\n",
    "            point (dict): Teleportable position to check.\n",
    "    \n",
    "        Returns:\n",
    "            bool: True if the point lies on the line, False otherwise.\n",
    "        \"\"\"\n",
    "        # Extract coordinates\n",
    "        ax, az = agent_position[\"x\"], agent_position[\"z\"]\n",
    "        tx, tz = target_position[\"x\"], target_position[\"z\"]\n",
    "        px, pz = point[\"x\"], point[\"z\"]\n",
    "    \n",
    "        # Check collinearity using the cross-product method\n",
    "        cross_product = abs((px - ax) * (tz - az) - (pz - az) * (tx - ax))\n",
    "        if cross_product > 1e-5:  # Allow for floating-point tolerance\n",
    "            return False\n",
    "    \n",
    "        # Check if the point is within the bounding box of the line segment\n",
    "        if min(ax, tx) <= px <= max(ax, tx) and min(az, tz) <= pz <= max(az, tz):\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "\n",
    "    \n",
    "    def _detect_objects(self, image, confidence_threshold=0.60):\n",
    "        \"\"\"Detect objects using Faster R-CNN.\"\"\"\n",
    "        image_width, image_height = image.size\n",
    "        tensor_image = to_tensor(image).unsqueeze(0)  # Correct usage\n",
    "\n",
    "        # Run Faster R-CNN\n",
    "        with torch.no_grad():\n",
    "            detections = self._frcnn_model(tensor_image)[0]\n",
    "\n",
    "        # Filter detections by confidence\n",
    "        valid_detections = [\n",
    "            {\n",
    "                \"x1\": box[0].item(),\n",
    "                \"y1\": box[1].item(),\n",
    "                \"x2\": box[2].item(),\n",
    "                \"y2\": box[3].item(),\n",
    "                \"confidence\": score.item(),\n",
    "            }\n",
    "            for box, score in zip(detections[\"boxes\"], detections[\"scores\"])\n",
    "            if score.item() >= confidence_threshold\n",
    "        ]\n",
    "        return valid_detections, image_width, image_height\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _classify_objects(self, detections, image, target_objects, padding=5):\n",
    "        \"\"\"Classify detected objects using CLIP.\"\"\"\n",
    "        image_width, image_height = image.size\n",
    "        detections_with_labels = []\n",
    "    \n",
    "        # Validate target_objects\n",
    "        if not isinstance(target_objects, dict) or \"target\" not in target_objects or \"context\" not in target_objects:\n",
    "            raise ValueError(\"The 'target_objects' dictionary must contain 'target' and 'context' keys.\")\n",
    "    \n",
    "        for det in detections:\n",
    "            try:\n",
    "                # Validate detection keys\n",
    "                if not all(key in det for key in [\"x1\", \"y1\", \"x2\", \"y2\"]):\n",
    "                    raise KeyError(f\"Detection is missing bounding box keys: {det}\")\n",
    "                \n",
    "                # Expand bounding box\n",
    "                x1, y1, x2, y2 = expand_box(\n",
    "                    (det[\"x1\"], det[\"y1\"], det[\"x2\"], det[\"y2\"]),\n",
    "                    image_width,\n",
    "                    image_height,\n",
    "                    padding,\n",
    "                )\n",
    "                cropped_image = image.crop((x1, y1, x2, y2))\n",
    "    \n",
    "                # Preprocess for CLIP\n",
    "                inputs = self._clip_processor(\n",
    "                    text=[target_objects[\"target\"]] + target_objects[\"context\"],\n",
    "                    images=cropped_image,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                )\n",
    "    \n",
    "                # Run CLIP classification\n",
    "                with torch.no_grad():\n",
    "                    outputs = self._clip_model(**inputs)\n",
    "                    logits_per_image = outputs.logits_per_image\n",
    "                    probs = logits_per_image.softmax(dim=1).squeeze(0)\n",
    "    \n",
    "                # Assign label\n",
    "                labels = [target_objects[\"target\"]] + target_objects[\"context\"]\n",
    "                max_prob_idx = torch.argmax(probs).item()\n",
    "                detected_label = labels[max_prob_idx]\n",
    "                confidence = probs[max_prob_idx].item()\n",
    "    \n",
    "                # Append result if label matches target_label\n",
    "                if detected_label == target_objects[\"target\"]:\n",
    "                    object_center_x = (x1 + x2) / 2\n",
    "                    detections_with_labels.append({\n",
    "                        \"label\": detected_label,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"center_x\": object_center_x,\n",
    "                        \"box\": (x1, y1, x2, y2)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error during object classification: {e}\")\n",
    "                continue\n",
    " \n",
    "        try:\n",
    "            detections_df = pd.DataFrame(detections_with_labels)\n",
    "            if detections_df.empty or \"label\" not in detections_df.columns:\n",
    "                return pd.DataFrame()  # Return an empty DataFrame\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame\n",
    "    \n",
    "        return detections_df\n",
    "\n",
    "\n",
    "    \n",
    "    def _find_target_angle_and_id_with_meta(self, target_name):\n",
    "        \"\"\"\n",
    "        Finds the best-matching object based on semantic similarity and calculates the turn angle.\n",
    "    \n",
    "        Args:\n",
    "            target_name (str): Name of the target object.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: (best_matching_object_id, turn_angle)\n",
    "        \"\"\"\n",
    "        # Step 1: Get visible objects\n",
    "        visible_objects = self._find_objects_in_sight(object_type=None)  \n",
    "\n",
    "        # Step 2: Find the floor object and extract receptacle object IDs\n",
    "        floor_object = next((obj for obj in visible_objects if obj['objectType'] == 'Floor'), None)\n",
    "        if not floor_object or 'receptacleObjectIds' not in floor_object:\n",
    "            return None, None\n",
    "    \n",
    "        receptacle_ids = floor_object['receptacleObjectIds']\n",
    "    \n",
    "        # Step 3: Filter visible objects to those whose IDs match receptacle IDs\n",
    "        filtered_objects = [obj for obj in visible_objects if obj['objectId'] in receptacle_ids]\n",
    "        logs=[]\n",
    "\n",
    "\n",
    "        best_match, logs = self._select_objects_by_similarity( logs, target_name, filtered_objects)\n",
    "\n",
    "    \n",
    "        if not best_match:\n",
    "            return None, None\n",
    "    \n",
    "        # Extract position of the best match\n",
    "        object_position = best_match['position']  \n",
    "    \n",
    "        # Step 5: Get agent's position and rotation \n",
    "        agent_position = self._metadata[-1]['agent']['position'] \n",
    "        agent_rotation = self._metadata[-1]['agent']['rotation']  \n",
    "        print(agent_position)\n",
    "        print(agent_rotation)\n",
    "        # Step 6: Calculate turn angle and distance \n",
    "        turn_angle, _ = calculate_turn_and_distance_dot_product(\n",
    "            (agent_position['x'], agent_position['z']),  # Use x and z for 2D position\n",
    "            agent_rotation,\n",
    "            (object_position['x'], object_position['z'])\n",
    "        )\n",
    "    \n",
    "        # Transform the turn angle into a single float value \n",
    "        turn_angle_float = turn_angle\n",
    "    \n",
    "        # Step 7: Return the object ID and turn angle\n",
    "        return best_match, turn_angle_float\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e388335-1073-4b4d-a92c-cbcca4edf9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variables\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d798c33-4703-4261-bd42-e55bf1c237fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/ComBots/thor-finds-thing/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/g/Documents/School_stuff/Master/ComBots/thor-finds-thing/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "EMISSOR_PATH = \"./emissor\"\n",
    "AGENT = \"Human\"\n",
    "HUMAN = \"AI2ThorCLient\"\n",
    "from leolani_client import LeolaniChatClient, Action\n",
    "thor = AI2ThorClient(leolaniClient=LeolaniChatClient(emissor_path=EMISSOR_PATH, agent=AGENT, human=HUMAN), chat_mode=\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff0977-486a-4a95-804e-15ab64653847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ef2dd-933b-4903-be23-7948b11a5475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f30fa-c15b-4ebe-88c2-3e59a46bed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "target='statue'\n",
    "context=['vase']\n",
    "logs=[]\n",
    "agent_info=[0, None, None]\n",
    "obj_id, logs, agent_info=thor._attempt_to_find_and_go_to_target(target, context, logs, agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88b1480-54f8-4389-8101-bca4e50fba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor._rotate('RotateLeft')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
