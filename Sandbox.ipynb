{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb8ed39-2cc8-417b-b8c7-35a0758079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64018c5",
   "metadata": {},
   "source": [
    "## Structured output testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7deec3a-7c3b-4709-bf71-49f913e953d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object_description\": {\n",
      "        \"name\": \"knife\",\n",
      "        \"position\": \"on a table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"context_description\": {\n",
      "        \"left_of_object\": \"TV\",\n",
      "        \"right_of_object\": \"two windows\",\n",
      "        \"behind_object\": null,\n",
      "        \"in_front_of_object\": null,\n",
      "        \"above_object\": null,\n",
      "        \"below_object\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"room_description\": {\n",
      "        \"room_type\": null,\n",
      "        \"size\": \"medium sized\",\n",
      "        \"additional_information\": [\n",
      "            \"It appears to be a kitchen or living room.\",\n",
      "            \"The table is far from the wall.\"\n",
      "        ]\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ContextDescription(BaseModel):\n",
    "    left_of_object: Optional[str]\n",
    "    right_of_object: Optional[str]\n",
    "    behind_object: Optional[str]\n",
    "    in_front_of_object: Optional[str]\n",
    "    above_object: Optional[str]\n",
    "    below_object: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object_description: Optional[ObjectDescription]\n",
    "    context_description: Optional[ContextDescription]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "    \n",
    "user_description= \"\"\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description.\"},\n",
    "        {\"role\": \"user\", \"content\": user_description}\n",
    "    ],\n",
    "    response_format=InitialDescription,\n",
    ")\n",
    "\n",
    "structured_description_struct_output = completion.choices[0].message.parsed\n",
    "print(json.dumps(structured_description_struct_output.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67805a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object_description\": {\n",
      "        \"name\": \"knife\",\n",
      "        \"position\": \"on a table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"context_description\": {\n",
      "        \"left_of_object\": \"TV\",\n",
      "        \"right_of_object\": \"two windows\",\n",
      "        \"behind_object\": \"table\",\n",
      "        \"in_front_of_object\": null,\n",
      "        \"above_object\": null,\n",
      "        \"below_object\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"room_description\": {\n",
      "        \"room_type\": \"Kitchen\",\n",
      "        \"size\": \"medium sized\",\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ContextDescription(BaseModel):\n",
    "    left_of_object: Optional[str]\n",
    "    right_of_object: Optional[str]\n",
    "    behind_object: Optional[str]\n",
    "    in_front_of_object: Optional[str]\n",
    "    above_object: Optional[str]\n",
    "    below_object: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object_description: Optional[ObjectDescription]\n",
    "    context_description: Optional[ContextDescription]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "llm = OpenAILlamaIndex()\n",
    "\n",
    "chat_prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage.from_str(\n",
    "            \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. Here is the description:\\n {movie_name}\", role=\"user\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_description_llamaindex = llm.structured_predict(\n",
    "    InitialDescription, chat_prompt_tmpl, movie_name=\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\n",
    ")\n",
    "\n",
    "print(json.dumps(structured_description_llamaindex.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91456c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object_description {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None} {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None}\n",
      "context_description {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': None, 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None} {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': 'table', 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None}\n",
      "room_description {'room_type': None, 'size': 'medium sized', 'additional_information': ['It appears to be a kitchen or living room.', 'The table is far from the wall.']} {'room_type': 'Kitchen', 'size': 'medium sized', 'additional_information': None}\n",
      "additional_information None None\n"
     ]
    }
   ],
   "source": [
    "# Compare structured_description_llamaindex and structured_description_struct_output by printing the values side by side\n",
    "for openai_key, llama_key in zip (structured_description_struct_output.model_dump().items(), structured_description_llamaindex.dict().items()):\n",
    "    print(openai_key[0], openai_key[1], llama_key[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6827b3",
   "metadata": {},
   "source": [
    "## AI2Thor wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "import ai2thor\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fa9180-6728-4c7b-a7cd-b5b11b991c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "class AI2ThorClient: \n",
    "    \"\"\"\n",
    "    An AI2Thor instance with methods wrapping its controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._controller = Controller(\n",
    "            agentMode=\"default\",\n",
    "            visibilityDistance=1.5,\n",
    "            scene=\"FloorPlan212\",\n",
    "\n",
    "            # step sizes\n",
    "            \n",
    "            gridSize=0.25,\n",
    "            snapToGrid=True,\n",
    "            rotateStepDegrees=90,\n",
    "\n",
    "            # image modalities\n",
    "            renderDepthImage=False,\n",
    "            renderInstanceSegmentation=False,\n",
    "\n",
    "            # camera properties\n",
    "            width=512,\n",
    "            height=512,\n",
    "            fieldOfView=90\n",
    "            )\n",
    "        self._events = []\n",
    "        self._llm_ollama = OllamaLlamaIndex(model=\"llama3.2\", request_timeout=120.0)\n",
    "        self._llm_openai = OpenAILlamaIndex(model=\"gpt-4o-2024-08-06\")\n",
    "        self._llm_openai_multimodal = OpenAI()\n",
    "        \n",
    "    def _get_image(self):\n",
    "        return Image.fromarray(self._controller.last_event.frame)\n",
    "    \n",
    "    def _rotate_to_coordinates(self, x, y, z):\n",
    "        \"\"\"AI2ThorClient\n",
    "        Rotates the agent to face the given coordinates.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            The x-coordinate to face.\n",
    "        y : float\n",
    "            The y-coordinate to face.\n",
    "        z : float\n",
    "            The z-coordinate to face.\n",
    "            \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def describe_scene_from_image(self):\n",
    "        \"\"\"\n",
    "        Describes the scene using an image-to-text model.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A string describing the current scene.\n",
    "        \"\"\"\n",
    "        \n",
    "        image = self._get_image()\n",
    "        \n",
    "        img_path  = f\"log/img/{str(time.time())}.jpg\"\n",
    "        image.save(img_path)\n",
    "        \n",
    "        encoded_image = encode_image(img_path)\n",
    "        \n",
    "        response = self._llm_openai_multimodal.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective so start your message with 'I'. First, describe the objects, their colors, and their positions. Don't introduce your description. Start describing directly e.g. 'I currently see a <object> on a <surface> ...'. Be objective in your description! Finally describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else. If you can't infer the room type, just say so.\",\n",
    "                        },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    \n",
    "    def step(self, direction: str = \"MoveAhead\", magnitude: float = None) -> ai2thor.server.Event:\n",
    "        \"\"\"\n",
    "        Robot takes one step in given direction.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            moveMagnitude=magnitude)\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def look(self, direction: str = \"LookUp\") -> None:\n",
    "        \"\"\"\n",
    "        Robot looks up or down.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=30\n",
    "            )\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def rotate(self, direction: str, degrees: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot turns in given direction.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        direction : str\n",
    "            Direction to turn in. Can be \"RotateLeft\" or \"RotateRight\".\n",
    "        \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=degrees\n",
    "            )\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def crouch(self):\n",
    "        \"\"\"\n",
    "        Robot crouches.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Crouch\")\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def stand(self):\n",
    "        \"\"\"\n",
    "        Robot stands.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Stand\")\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "        \n",
    "    def teleport(self, position: dict = None, rotation: dict = None, horizon: float = None, standing: bool = None, to_random: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Robot teleports to random location.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        \n",
    "        if to_random:\n",
    "            rotation = {\"x\": random.randint(0, 360), \"y\": random.randint(0, 360), \"z\": random.randint(0, 360)}\n",
    "            positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "            position = random.choice(positions)\n",
    "            \n",
    "        self._controller.step(\n",
    "            action=\"Teleport\",\n",
    "            position=position,\n",
    "            rotation=rotation,\n",
    "            horizon=horizon,\n",
    "            standing=standing\n",
    "        )\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def done(self) -> None:\n",
    "        \"\"\"\n",
    "        The Done action does nothing to the state of the environment. \n",
    "        But, it returns a cleaned up event with respect to the metadata.\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Done\")\n",
    "        \n",
    "        self._events.append(self._controller.last_event.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494431fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor = AI2ThorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5297c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor.teleport(to_random=True)\n",
    "thor.rotate(direction=\"RotateRight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43fa3543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm currently seeing a green sofa with three cushions on it. To the right of the sofa, there's a black remote placed on the seat. A red cushion is also set on the left cushion of the sofa. There are two more chairs, each with a green cushion and positioned on either side of the sofa. In the background, I notice two vertical windows allowing natural light to enter the room. I would identify this area as a living room.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor.describe_scene_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901a15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArmChair|-00.27|+00.00|+01.87\n",
      "ArmChair|+02.66|+00.00|+01.86\n",
      "Boots|+04.00|+00.00|+01.70\n",
      "Box|-00.47|+01.04|-00.71\n",
      "CoffeeTable|+01.59|00.00|+00.45\n",
      "CreditCard|+01.41|+00.47|+00.65\n",
      "Drawer|+03.88|+00.77|+00.86\n",
      "Floor|+00.00|+00.00|+00.00\n",
      "FloorLamp|+03.61|+00.00|+02.16\n",
      "GarbageCan|+03.83|-00.03|-00.50\n",
      "HousePlant|+00.39|+00.80|-00.73\n",
      "KeyChain|+01.50|+00.47|+00.53\n",
      "Laptop|+01.80|+00.47|+00.50\n",
      "LightSwitch|-01.40|+01.29|+01.84\n",
      "Newspaper|+02.15|+00.41|-00.72\n",
      "Painting|+04.07|+01.95|+00.85\n",
      "Pen|+03.93|+00.87|+01.04\n",
      "Pencil|+03.89|+00.87|+01.18\n",
      "Pillow|+00.65|+00.39|+01.71\n",
      "RemoteControl|+01.88|+00.33|+01.73\n",
      "Shelf|-00.29|+00.59|-00.73\n",
      "Shelf|+01.91|+00.20|-00.73\n",
      "Shelf|-00.29|+00.20|-00.73\n",
      "Shelf|+01.91|+00.59|-00.73\n",
      "SideTable|+03.95|+00.00|+00.86\n",
      "Sofa|+01.19|+00.01|+01.87\n",
      "Statue|-00.09|+00.03|-00.70\n",
      "Statue|-00.54|+00.40|-00.69\n",
      "Television|+01.90|+01.28|-00.84\n",
      "TissueBox|+03.92|+00.87|+00.68\n",
      "TVStand|-00.29|00.00|-00.77\n",
      "TVStand|+01.90|00.00|-00.77\n",
      "WateringCan|+01.62|+00.02|-00.70\n",
      "Window|+01.57|+02.07|+02.49\n",
      "Window|+00.02|+02.07|+02.49\n"
     ]
    }
   ],
   "source": [
    "thor.teleport(to_random=True)\n",
    "last_event = thor._events[-1]\n",
    "objects = last_event[\"objects\"]\n",
    "\n",
    "for obj in objects:\n",
    "    print(obj[\"objectId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb43c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\", additional_kwargs={'tool_calls': []}, raw={'model': 'llama3.2', 'created_at': '2024-11-15T09:08:54.133358Z', 'message': {'role': 'assistant', 'content': \"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 5177552708, 'load_duration': 3339315041, 'prompt_eval_count': 30, 'prompt_eval_duration': 79901000, 'eval_count': 91, 'eval_duration': 1756324000, 'usage': {'prompt_tokens': 30, 'completion_tokens': 91, 'total_tokens': 121}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor._llm_ollama.complete(\"What do you see?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb1090",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1dca3b-b338-4146-b079-0586c0653dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found the object!\n"
     ]
    }
   ],
   "source": [
    "class InitialDescriptionComplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class InitialDescriptionIncomplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectFound(Event):\n",
    "    payload: str\n",
    "\n",
    "class WrongObjectSuggested(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomCorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomIncorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectNotInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "number = 1\n",
    "\n",
    "class ThorFindsObject(Workflow):\n",
    "    \n",
    "    def __init__(self, timeout: int = 10, verbose: bool = False):\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "        self.thor = AI2ThorClient()\n",
    "    \n",
    "    @step\n",
    "    async def evaluate_initial_description(self, ev: StartEvent) -> InitialDescriptionComplete | InitialDescriptionIncomplete:\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return InitialDescriptionComplete(payload=\"Initial description is complete.\")\n",
    "        else:\n",
    "            return InitialDescriptionIncomplete(payload=\"Initial description is incomplete.\")\n",
    "\n",
    "    @step\n",
    "    async def clarify_initial_description(self, ev: InitialDescriptionIncomplete) -> InitialDescriptionComplete:\n",
    "        return InitialDescriptionComplete(payload=\"Description clarified.\")\n",
    "\n",
    "    @step\n",
    "    async def find_correct_room_type(self, ev: InitialDescriptionComplete | RoomIncorrect | ObjectNotInRoom) -> RoomCorrect | RoomIncorrect:\n",
    "        current_description = \"\"\"a living room setup viewed from behind a dark-colored couch. The room has light-colored walls and a floor that seems to be a muted, earthy tone. The main items in the room include:\n",
    "- A large, dark-colored sofa in the foreground facing a TV.\n",
    "- A television placed on a small white TV stand, positioned along the far wall.\n",
    "- A small side table with a blue vase and a decorative item beside the TV stand.\n",
    "- A wooden shelf or cabinet off to the left side of the room.\"\"\"\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return RoomCorrect(payload=\"Correct room is found.\")\n",
    "        else:\n",
    "            return RoomIncorrect(payload=\"Correct room is not found.\")\n",
    "\n",
    "    @step \n",
    "    async def find_object_in_room(self, ev: RoomCorrect) -> ObjectInRoom | ObjectNotInRoom:\n",
    "        if random.randint(0, 10) < 4:\n",
    "            return ObjectInRoom(payload=\"Object may be in this room.\")\n",
    "        else:\n",
    "            return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "    \n",
    "    @step \n",
    "    async def suggest_object(self, ev: ObjectInRoom | WrongObjectSuggested) -> WrongObjectSuggested | ObjectNotInRoom | StopEvent:\n",
    "        \n",
    "        \n",
    "        if  random.randint(0, 1) == 0:\n",
    "            return StopEvent(result=\"We found the object!\")  # End the workflow\n",
    "        else:\n",
    "            if random.randint(0, 1) == 0:\n",
    "                return WrongObjectSuggested(payload=\"Couldn't find object in this room.\")\n",
    "            else:\n",
    "                return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Initialize and run the workflow\n",
    "w = ThorFindsObject(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81d8f70-1c60-4eba-87b3-daf0233f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionIncomplete'>\n",
      "<class '__main__.RoomCorrect'>\n",
      "<class '__main__.RoomIncorrect'>\n",
      "<class '__main__.ObjectInRoom'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class '__main__.WrongObjectSuggested'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "possible_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(ThorFindsObject, filename=\"possible_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbad389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chainlit in ./myenv/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2.1)\n",
      "Requirement already satisfied: asyncer<0.0.8,>=0.0.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.7)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: dataclasses_json<0.7.0,>=0.6.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.6.7)\n",
      "Requirement already satisfied: fastapi<0.116,>=0.115.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.115.4)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.27.2)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: literalai==0.0.623 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.623)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.0.1)\n",
      "Requirement already satisfied: python-multipart<0.0.10,>=0.0.9 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.9)\n",
      "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (5.11.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.41.2 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.41.2)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.2)\n",
      "Requirement already satisfied: uptrace<2.0.0,>=1.22.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.27.0)\n",
      "Requirement already satisfied: uvicorn<0.26.0,>=0.25.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.25.0)\n",
      "Requirement already satisfied: watchfiles<0.21.0,>=0.20.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.20.0)\n",
      "Requirement already satisfied: chevron>=0.14.0 in ./myenv/lib/python3.11/site-packages (from literalai==0.0.623->chainlit) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in ./myenv/lib/python3.11/site-packages (from asyncer<0.0.8,>=0.0.7->chainlit) (4.6.2.post1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.11/site-packages (from fastapi<0.116,>=0.115.3->chainlit) (4.12.2)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.0.6)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (3.10)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (2.23.4)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.10.1)\n",
      "Requirement already satisfied: opentelemetry-api~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation~=0.48b0 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: requests~=2.7 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-proto==1.28.0->opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.16.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in ./myenv/lib/python3.11/site-packages (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./myenv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.7.0,>=0.6.7->chainlit) (1.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.2)\n",
      "Requirement already satisfied: wsproto in ./myenv/lib/python3.11/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d5f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
