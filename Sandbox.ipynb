{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb8ed39-2cc8-417b-b8c7-35a0758079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    "    Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64018c5",
   "metadata": {},
   "source": [
    "## Structured output testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7deec3a-7c3b-4709-bf71-49f913e953d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:48:02.803924Z",
     "start_time": "2024-12-06T08:47:57.334383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"target_object\": {\n",
      "        \"name\": \"Knife\",\n",
      "        \"position\": \"On the table\",\n",
      "        \"size\": null,\n",
      "        \"texture\": null,\n",
      "        \"color\": null,\n",
      "        \"additional_information\": null\n",
      "    },\n",
      "    \"object_in_context\": [\n",
      "        {\n",
      "            \"name\": \"Table\",\n",
      "            \"position\": \"Far from the wall\",\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": null,\n",
      "            \"position_relative_to_target_object\": \"The knife is on the table\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"TV\",\n",
      "            \"position\": null,\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": null,\n",
      "            \"position_relative_to_target_object\": \"Left of the table which the knife is on\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Windows\",\n",
      "            \"position\": \"On the wall behind the table\",\n",
      "            \"size\": null,\n",
      "            \"texture\": null,\n",
      "            \"color\": null,\n",
      "            \"additional_information\": [\n",
      "                \"There are two windows\"\n",
      "            ],\n",
      "            \"position_relative_to_target_object\": \"Right of the table which the knife is on\"\n",
      "        }\n",
      "    ],\n",
      "    \"room_description\": {\n",
      "        \"room_type\": null,\n",
      "        \"size\": \"Medium\",\n",
      "        \"additional_information\": [\n",
      "            \"It appears to be a kitchen or living room\"\n",
      "        ]\n",
      "    },\n",
      "    \"additional_information\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from typing import Literal\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "class ContextObjectDescription(ObjectDescription):\n",
    "    position_relative_to_target_object: Optional[str]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object: Optional[ObjectDescription]\n",
    "    object_in_context: Optional[List[ContextObjectDescription]]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "    \n",
    "user_description= \"\"\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\"\"\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description.\"},\n",
    "        {\"role\": \"user\", \"content\": user_description}\n",
    "    ],\n",
    "    response_format=InitialDescription,\n",
    ")\n",
    "\n",
    "structured_description_struct_output = completion.choices[0].message.parsed\n",
    "print(json.dumps(structured_description_struct_output.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67805a33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:48:05.397754Z",
     "start_time": "2024-12-06T08:48:03.011395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object=ObjectDescription(name='knife', position='on a table', size='N/A', texture='N/A', color='N/A', additional_information=[]) object_in_context=[ContextObjectDescription(name='TV', position='left of the table', size='N/A', texture='N/A', color='N/A', additional_information=None, position_relative_to_target_object=None), ContextObjectDescription(name='windows', position='on the wall behind the table', size='N/A', texture='N/A', color='N/A', additional_information=None, position_relative_to_target_object=None)] room_description=RoomDescription(room_type='Kitchen', size='medium sized', additional_information=[]) additional_information=[]\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, List\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class ObjectDescription(BaseModel):\n",
    "    name: Optional[str]\n",
    "    position: Optional[str]\n",
    "    size: Optional[str]\n",
    "    texture: Optional[str]\n",
    "    color: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "class ContextObjectDescription(ObjectDescription):\n",
    "    position_relative_to_target_object: Optional[str]\n",
    "\n",
    "class RoomDescription(BaseModel):\n",
    "    room_type: Optional[Literal[\"Living room\", \"Kitchen\", \"Bedroom\", \"Bathroom\"]]\n",
    "    size: Optional[str]\n",
    "    additional_information: Optional[List[str]]\n",
    "\n",
    "class InitialDescription(BaseModel):\n",
    "    target_object: Optional[ObjectDescription]\n",
    "    object_in_context: Optional[List[ContextObjectDescription]]\n",
    "    room_description: Optional[RoomDescription]\n",
    "    additional_information: Optional[List[str]]\n",
    "    \n",
    "llm = OpenAILlamaIndex()\n",
    "\n",
    "chat_prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage.from_str(\n",
    "            \"Your task is to turn a user's description of an object, its context and the room type into a structured response. When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. Here is the description:\\n {movie_name}\", role=\"user\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "structured_description_llamaindex = llm.structured_predict(\n",
    "    InitialDescription, chat_prompt_tmpl, movie_name=\"The object is a knife, it's on a table in a medium sized room. The table is far from the wall. It appears to be a kitchen or living room. Left of the table which the knife is one is a TV and on the right on the wall behind the table are two windows.\"\n",
    ")\n",
    "\n",
    "print(structured_description_llamaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91456c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_object_description {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None} {'name': 'knife', 'position': 'on a table', 'size': None, 'texture': None, 'color': None, 'additional_information': None}\n",
      "context_description {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': None, 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None} {'left_of_object': 'TV', 'right_of_object': 'two windows', 'behind_object': 'table', 'in_front_of_object': None, 'above_object': None, 'below_object': None, 'additional_information': None}\n",
      "room_description {'room_type': None, 'size': 'medium sized', 'additional_information': ['It appears to be a kitchen or living room.', 'The table is far from the wall.']} {'room_type': 'Kitchen', 'size': 'medium sized', 'additional_information': None}\n",
      "additional_information None None\n"
     ]
    }
   ],
   "source": [
    "# Compare structured_description_llamaindex and structured_description_struct_output by printing the values side by side\n",
    "for openai_key, llama_key in zip (structured_description_struct_output.model_dump().items(), structured_description_llamaindex.dict().items()):\n",
    "    print(openai_key[0], openai_key[1], llama_key[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6827b3",
   "metadata": {},
   "source": [
    "## AI2Thor wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a715eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "import ai2thor\n",
    "import random\n",
    "from PIL import Image\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "42fa9180-6728-4c7b-a7cd-b5b11b991c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor.controller import Controller\n",
    "from PIL import Image\n",
    "from llama_index.llms.openai import OpenAI as OpenAILlamaIndex\n",
    "from llama_index.llms.ollama import Ollama as OllamaLlamaIndex\n",
    "from descriptions import InitialDescription, ViewDescription\n",
    "from leolani_client import Action\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "from thor_utils import ( \n",
    "                        encode_image, \n",
    "                        get_distance,\n",
    "                        closest_objects\n",
    "                       )\n",
    "\n",
    "# Constants\n",
    "VISIBILITY_DISTANCE = 1.5\n",
    "SCENE = \"FloorPlan212\"\n",
    "\n",
    "class AI2ThorClient: \n",
    "    \"\"\"\n",
    "    An AI2Thor instance with methods wrapping its controller.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, leolaniClient, chat_mode):\n",
    "        self._controller = Controller(\n",
    "            agentMode=\"default\",\n",
    "            visibilityDistance=VISIBILITY_DISTANCE,\n",
    "            scene=SCENE,\n",
    "\n",
    "            # step sizes\n",
    "            gridSize=0.25,\n",
    "            snapToGrid=True,\n",
    "            rotateStepDegrees=90,\n",
    "\n",
    "            # image modalities\n",
    "            renderDepthImage=False,\n",
    "            renderInstanceSegmentation=False,\n",
    "\n",
    "            # camera properties\n",
    "            width=512,\n",
    "            height=512,\n",
    "            fieldOfView=90\n",
    "            )\n",
    "        self._metadata = []\n",
    "        self.descriptions = []\n",
    "        self.unstructured_descriptions = []\n",
    "        self.leolaniClient = leolaniClient\n",
    "        self._llm_ollama = OllamaLlamaIndex(model=\"llama3.2\", request_timeout=120.0)\n",
    "        self._llm_openai = OpenAILlamaIndex(model=\"gpt-4o-2024-08-06\")\n",
    "        self._llm_openai_multimodal = OpenAI()\n",
    "\n",
    "\n",
    "    def describe_view_from_image(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A string describing the current view.\n",
    "        \"\"\"\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "\n",
    "        response = self._llm_openai_multimodal.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective so start your message with 'I'. First, describe the objects, their colors, and their positions. Don't introduce your description. Start describing directly e.g. 'I currently see a <object> on a <surface> ...'. Be objective in your description! Finally describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else. If you can't infer the room type, just say so.\",\n",
    "                        },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            },\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.content)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "    def describe_view_from_image_structured(self):\n",
    "        \"\"\"\n",
    "        Describes the current view using an image-to-text model with structure.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        ViewDescription\n",
    "            A structured description of the current view.\n",
    "        \"\"\"    \n",
    "\n",
    "        encoded_image = encode_image(self._get_image())\n",
    "        \n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Imagine this is your point-of-view. Describe what you see in this virtual environment. Write from the first perspective. Describe the objects, their colors, and their positions. Be objective in your description! Describe the room type: it's either a living room, kitchen, bedroom, or bedroom. It can't be anything else.\",\n",
    "                            },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\":  f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            response_format=ViewDescription,\n",
    "            )\n",
    "        \n",
    "        self.descriptions.append(response.choices[0].message.parsed)\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "    def infer_room_type(self, description: str) -> str:\n",
    "        \"\"\"\n",
    "        Infers the room type the agent is in.\n",
    "\n",
    "        Inference is based on:\n",
    "        - The image-to-text description of the view.\n",
    "        - The objects in the metadata.\n",
    "        - The AI2Thor object types mapping (https://ai2thor.allenai.org/ithor/documentation/objects/object-types).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns a string representing the likely room type.\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n",
    "    def parse_unstructured_description(self, description: str):\n",
    "        \"\"\"\n",
    "        Parse an unstructured description into structured data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        description : str\n",
    "            The unstructured description to parse.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        PydanticModel\n",
    "            An instance of the given Pydantic model populated with the parsed data.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self._llm_openai_multimodal.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-2024-08-06\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"Your task is to turn a user's description of an object, its context and the room type into a structured response. \n",
    "                 When information is missing from the user's description, do not make up parts of the description, go ONLY off of the user's description. \n",
    "                 Only deviate from this rule when positions of objects in context are obvious, such as a floor (which is always below the target object) and a ceiling (which is above).\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ],\n",
    "            response_format=InitialDescription,\n",
    "        )\n",
    "\n",
    "        self.structured_initial_description = response.choices[0].message.parsed\n",
    "\n",
    "    def _get_image(self):\n",
    "        image = Image.fromarray(self._controller.last_event.frame)\n",
    "        # self.leolaniClient._add_image()\n",
    "        return image\n",
    "    \n",
    "        \n",
    "\n",
    "    def _step(self, direction: str = \"MoveAhead\", magnitude: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot takes one step in given direction. Options are:\n",
    "            - MoveAhead\n",
    "            - MoveBack\n",
    "            - MoveLeft\n",
    "            - MoveRight\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            moveMagnitude=magnitude\n",
    "            ) \n",
    "\n",
    "        action_attribute = getattr(Action, direction, None)\n",
    "        if action_attribute is not None:\n",
    "            self.leolaniClient._add_action(action_attribute)\n",
    "        else:\n",
    "            raise AttributeError(f\"'Action' object has no attribute '{direction}'\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _look(self, direction: str = \"LookUp\") -> None:\n",
    "        \"\"\"\n",
    "        Robot looks up or down. Options are:\n",
    "        - LookUp\n",
    "        - LookDown\n",
    "    \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=30\n",
    "            )\n",
    "\n",
    "        self.leolaniClient._add_action(Action.direction)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _rotate(self, direction: str, degrees: float = None) -> None:\n",
    "        \"\"\"\n",
    "        Robot turns in given direction (for optional degrees).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        direction : str\n",
    "            Direction to turn in. Can be \"RotateLeft\" or \"RotateRight\".\n",
    "        degrees : float, optional\n",
    "            Degrees to turn. Default is None.\n",
    "        \n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(\n",
    "            action=direction,\n",
    "            degrees=degrees\n",
    "            )\n",
    "        \n",
    "        if direction == \"RotateLeft\":\n",
    "            self.leolaniClient._add_action(Action.RotateLeft)\n",
    "        elif direction == \"RotateRight\":\n",
    "            self.leolaniClient._add_action(Action.RotateRight)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _crouch(self):\n",
    "        \"\"\"\n",
    "        Robot crouches.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Crouch\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Crouch)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _stand(self):\n",
    "        \"\"\"\n",
    "        Robot stands.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Stand\")\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Stand)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "\n",
    "    def _teleport(self, position: dict = None, rotation: dict = None, horizon: float = None, standing: bool = None, to_random: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Robot teleports to random location.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        position: dict\n",
    "            The 'x', 'y', 'z' coordinates.\n",
    "        rotation: num\n",
    "            The rotation of the agent's body. If unspecified, the rotation of the agent remains the same.\n",
    "        horizon: Float\n",
    "            Look up of down. Negative values (e.g. -30) correspond to agent looking up, and vice versa.\n",
    "        standing: bool\n",
    "            True for \n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "\n",
    "        if to_random:\n",
    "            rotation = dict(x=0, y=random.randint(0, 360), z=0)\n",
    "            reachable_positions = self._controller.step(action=\"GetReachablePositions\").metadata[\"actionReturn\"]\n",
    "            position = random.choice(reachable_positions)\n",
    "        \n",
    "        params = {\"action\": \"Teleport\", \"position\": position}\n",
    "        if rotation is not None:\n",
    "            params[\"rotation\"] = rotation\n",
    "        if horizon is not None:\n",
    "            params[\"horizon\"] = horizon\n",
    "        if standing is not None:\n",
    "            params[\"standing\"] = standing\n",
    "            \n",
    "        self._controller.step(**params)\n",
    "\n",
    "        self.leolaniClient._add_action(Action.Teleport)\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n",
    "    \n",
    "    def _find_objects_in_sight(self, object_type: str = None) -> list:\n",
    "        \"\"\"\n",
    "        Finds objects in sight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object_type : str\n",
    "            The type of object to find.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of objects in sight.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get objects in sight\n",
    "        objects_in_sight = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"visible\"] == True]\n",
    "\n",
    "        # Optionally filter by object type\n",
    "        if object_type:\n",
    "            objects_in_sight = [obj for obj in objects_in_sight if obj[\"objectType\"] == object_type]\n",
    "\n",
    "        return objects_in_sight\n",
    "    \n",
    "    def _find_all_rooms(self, number=None):\n",
    "        \"\"\"\n",
    "        Create a list of all rooms (based on `roomType` == \"Floor\") in current scene. \n",
    "        Sorted from nearest to furthest.\n",
    "        \n",
    "        \"\"\"\n",
    "        rooms = [obj for obj in self._controller.last_event.metadata[\"objects\"] if obj[\"objectType\"] == \"Floor\"]\n",
    "        rooms.sort(key=lambda room: room['distance'])\n",
    "        return rooms\n",
    "        \n",
    "    def _find_nearest_center_of_room(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary with \"x\", \"y\", \"z\" coordinates of nearest center of room(s).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        \"\"\"\n",
    "        rooms = self._find_all_rooms()\n",
    "        nearest_room = rooms[0]\n",
    "        center = nearest_room['axisAlignedBoundingBox']['center']\n",
    "        return center\n",
    "\n",
    "    def _done(self) -> None:\n",
    "        \"\"\"\n",
    "        The Done action does nothing to the state of the environment. \n",
    "        But, it returns a cleaned up event with respect to the metadata.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        self._controller.step(action=\"Done\")\n",
    "\n",
    "        self._metadata.append(self._controller.last_event.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "494431fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMISSOR_PATH = \"./emissor\"\n",
    "AGENT = \"Human\"\n",
    "HUMAN = \"AI2ThorCLient\"\n",
    "from leolani_client import LeolaniChatClient, Action\n",
    "thor = AI2ThorClient(leolaniClient=LeolaniChatClient(emissor_path=EMISSOR_PATH, agent=AGENT, human=HUMAN), chat_mode=\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b5297c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor._teleport(position={\"x\":0, \"y\": 0.9009991884231567, \"z\": 0})\n",
    "thor._teleport(position={\"x\":1, \"y\": 0.9009991884231567, \"z\": 1})\n",
    "thor._teleport(position={\"x\":2, \"y\": 0.9009991884231567, \"z\": 2})\n",
    "thor._teleport(position={\"x\":3, \"y\": 0.9009991884231567, \"z\": 3})\n",
    "thor._teleport(to_random=True)\n",
    "thor._teleport(position=thor._find_nearest_center_of_room())\n",
    "# thor._rotate(direction=\"RotateRight\")\n",
    "# thor._step(direction=\"MoveAhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0c6c56a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently see a brown cardboard box on a reddish-brown wooden surface, which appears to be a table or cabinet. There is a shiny, metallic object close to the box. To my right, there is a light gray wall with a white door that has a gold handle. Beyond this door, there is another room with a dark brown wall and a white door at the end. The floor is light wood, and the lighting is soft, creating a calm atmosphere. The space suggests it might be a living room.\n"
     ]
    }
   ],
   "source": [
    "thor._find_objects_in_sight()\n",
    "print(thor.describe_view_from_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d4f9aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "thor._rotate(direction=\"RotateLeft\")\n",
    "thor._stand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a38d81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -4.0, 'y': 0.9009991884231567, 'z': -1.0}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -2.0, 'y': 0.9009991884231567, 'z': -0.5}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.5, 'y': 0.9009991884231567, 'z': 0.25}\n",
      "Teleport\n",
      "{'x': 0.0, 'y': 0.9009991884231567, 'z': 0.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': 1.0, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "Teleport\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n",
      "RotateLeft\n",
      "{'x': -0.75, 'y': 0.9009991884231567, 'z': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for metadata in thor._metadata:\n",
    "    print(metadata['lastAction'])\n",
    "    print(metadata['agent']['position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43fa3543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I currently see a dark blue couch positioned against a light-colored wall. There is a plant peeking into view from the corner of the image. On the wall directly in front of me, there's a colorful abstract painting. The floor is made of wooden planks, giving a warm, inviting appearance. In the background, I notice a white door and what seems to be a hallway or another room. The overall arrangement and items present suggest that this is a living room.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor.describe_view_from_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dd651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Floor_e3656fd3', 'position': {'x': 0.0, 'y': 0.0, 'z': 0.0}, 'rotation': {'x': -0.0, 'y': 0.0, 'z': 0.0}, 'visible': True, 'isInteractable': True, 'receptacle': True, 'toggleable': False, 'isToggled': False, 'breakable': False, 'isBroken': False, 'canFillWithLiquid': False, 'isFilledWithLiquid': False, 'fillLiquid': None, 'dirtyable': False, 'isDirty': False, 'canBeUsedUp': False, 'isUsedUp': False, 'cookable': False, 'isCooked': False, 'temperature': 'Cold', 'isHeatSource': False, 'isColdSource': False, 'sliceable': False, 'isSliced': False, 'openable': False, 'isOpen': False, 'openness': 0.0, 'pickupable': False, 'isPickedUp': False, 'moveable': False, 'mass': 0.0, 'salientMaterials': None, 'receptacleObjectIds': ['Box|-02.00|+00.24|-02.66', 'SideTable|-05.73|+00.00|-01.28', 'Bed|-04.98|+00.00|-02.36', 'BaseballBat|-05.76|+00.14|-00.41', 'Doorway|-02.24|-00.01|+00.04', 'Desk|-05.52|00.00|-00.30', 'Chair|-04.43|00.00|-00.34', 'Shelf|-05.52|+00.10|-00.30', 'BasketBall|-02.46|+00.12|-02.28', 'Plunger|-01.88|00.00|-02.12', 'ScrubBrush|-01.67|+00.00|-02.17', 'ShowerCurtain|-01.25|+01.94|-02.49', 'Toilet|-00.35|+00.00|-01.89', 'Doorway|-00.88|+00.00|+00.03', 'CounterTop|-00.13|+00.00|-00.71', 'DogBed|-05.80|+00.00|+02.74', 'Drawer|-05.52|+00.23|+01.95', 'CoffeeTable|-05.52|+00.00|+01.76', 'Drawer|-05.52|+00.07|+01.95', 'Drawer|-05.52|+00.07|+01.57', 'Drawer|-05.52|+00.23|+01.57', 'ArmChair|-03.91|+00.00|+00.82', 'Doorway|-03.25|-00.01|+01.00', 'SideTable|-04.79|+00.00|+00.63', 'Shelf|-04.79|+00.26|+00.63', 'FloorLamp|+04.65|+00.00|+02.61', 'SideTable|+03.70|+00.00|+02.64', 'Shelf|+03.70|+00.26|+02.64', 'TVStand|+02.56|+00.00|+02.69', 'Shelf|+02.56|+00.24|+02.69', 'CoffeeTable|+03.57|+00.00|-00.12', 'Sofa|+02.42|+00.00|-00.11', 'Doorway|+06.24|+00.00|-00.27', 'Stool|+05.76|+00.00|-00.93', 'Stool|+05.22|+00.00|-01.58', 'DiningTable|+05.76|+00.00|-01.69', 'Stool|+05.70|+00.00|-02.66', 'HousePlant|+03.62|+00.13|-00.10', 'GarbageCan|+03.65|00.00|-02.92', 'CounterTop|+02.40|+00.00|-01.02', 'CounterTop|+02.53|+00.00|-02.81', 'Fridge|+01.00|+00.00|-02.79'], 'distance': 3.8889973163604736, 'objectType': 'Floor', 'objectId': 'Floor|+00.00|+00.00|+00.00', 'assetId': '', 'parentReceptacles': None, 'controlledObjects': None, 'isMoving': False, 'axisAlignedBoundingBox': {'cornerPoints': [[6.2437944412231445, 0.0, 3.2035932540893555], [6.2437944412231445, 0.0, -3.2452797889709473], [6.2437944412231445, -0.3243083953857422, 3.2035932540893555], [6.2437944412231445, -0.3243083953857422, -3.2452797889709473], [-6.2437944412231445, 0.0, 3.2035932540893555], [-6.2437944412231445, 0.0, -3.2452797889709473], [-6.2437944412231445, -0.3243083953857422, 3.2035932540893555], [-6.2437944412231445, -0.3243083953857422, -3.2452797889709473]], 'center': {'x': 0.0, 'y': -0.1621541976928711, 'z': -0.0208432674407959}, 'size': {'x': 12.487588882446289, 'y': 0.3243083953857422, 'z': 6.448873043060303}}, 'objectOrientedBoundingBox': None}]\n"
     ]
    }
   ],
   "source": [
    "rooms = [obj for obj in thor._metadata[-1][\"objects\"] if obj[\"objectType\"] == \"Floor\"]\n",
    "rooms.sort(key=lambda room: room['distance'])\n",
    "\n",
    "print(rooms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "97a44cc974298805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0.0, 'y': -0.1621541976928711, 'z': -0.0208432674407959}\n"
     ]
    }
   ],
   "source": [
    "print(rooms[0]['axisAlignedBoundingBox']['center'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c0e15ce3df295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901a15fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArmChair|-00.27|+00.00|+01.87\n",
      "ArmChair|+02.66|+00.00|+01.86\n",
      "Boots|+04.00|+00.00|+01.70\n",
      "Box|-00.47|+01.04|-00.71\n",
      "CoffeeTable|+01.59|00.00|+00.45\n",
      "CreditCard|+01.41|+00.47|+00.65\n",
      "Drawer|+03.88|+00.77|+00.86\n",
      "Floor|+00.00|+00.00|+00.00\n",
      "FloorLamp|+03.61|+00.00|+02.16\n",
      "GarbageCan|+03.83|-00.03|-00.50\n",
      "HousePlant|+00.39|+00.80|-00.73\n",
      "KeyChain|+01.50|+00.47|+00.53\n",
      "Laptop|+01.80|+00.47|+00.50\n",
      "LightSwitch|-01.40|+01.29|+01.84\n",
      "Newspaper|+02.15|+00.41|-00.72\n",
      "Painting|+04.07|+01.95|+00.85\n",
      "Pen|+03.93|+00.87|+01.04\n",
      "Pencil|+03.89|+00.87|+01.18\n",
      "Pillow|+00.65|+00.39|+01.71\n",
      "RemoteControl|+01.88|+00.33|+01.73\n",
      "Shelf|-00.29|+00.59|-00.73\n",
      "Shelf|+01.91|+00.20|-00.73\n",
      "Shelf|-00.29|+00.20|-00.73\n",
      "Shelf|+01.91|+00.59|-00.73\n",
      "SideTable|+03.95|+00.00|+00.86\n",
      "Sofa|+01.19|+00.01|+01.87\n",
      "Statue|-00.09|+00.03|-00.70\n",
      "Statue|-00.54|+00.40|-00.69\n",
      "Television|+01.90|+01.28|-00.84\n",
      "TissueBox|+03.92|+00.87|+00.68\n",
      "TVStand|-00.29|00.00|-00.77\n",
      "TVStand|+01.90|00.00|-00.77\n",
      "WateringCan|+01.62|+00.02|-00.70\n",
      "Window|+01.57|+02.07|+02.49\n",
      "Window|+00.02|+02.07|+02.49\n"
     ]
    }
   ],
   "source": [
    "thor.teleport(to_random=True)\n",
    "last_event = thor._events[-1]\n",
    "objects = last_event[\"objects\"]\n",
    "\n",
    "for obj in objects:\n",
    "    print(obj[\"objectId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02cb43c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\", additional_kwargs={'tool_calls': []}, raw={'model': 'llama3.2', 'created_at': '2024-11-15T09:08:54.133358Z', 'message': {'role': 'assistant', 'content': \"I don't have the ability to see or perceive the physical world. I'm a text-based AI assistant, and my interactions are limited to processing and generating text. I can understand and respond to text-based inputs, but I don't have visual capabilities or direct access to sensory information.\\n\\nHowever, I can help facilitate discussions about what you see, provide information about visual topics, or assist with tasks that involve analyzing images or videos if you'd like.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 5177552708, 'load_duration': 3339315041, 'prompt_eval_count': 30, 'prompt_eval_duration': 79901000, 'eval_count': 91, 'eval_duration': 1756324000, 'usage': {'prompt_tokens': 30, 'completion_tokens': 91, 'total_tokens': 121}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thor._llm_ollama.complete(\"What do you see?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb1090",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1dca3b-b338-4146-b079-0586c0653dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found the object!\n"
     ]
    }
   ],
   "source": [
    "class InitialDescriptionComplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class InitialDescriptionIncomplete(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectFound(Event):\n",
    "    payload: str\n",
    "\n",
    "class WrongObjectSuggested(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomCorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class RoomIncorrect(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "class ObjectNotInRoom(Event):\n",
    "    payload: str\n",
    "\n",
    "number = 1\n",
    "\n",
    "class ThorFindsObject(Workflow):\n",
    "    \n",
    "    def __init__(self, timeout: int = 10, verbose: bool = False):\n",
    "        super().__init__(timeout=timeout, verbose=verbose)\n",
    "        self.thor = AI2ThorClient()\n",
    "    \n",
    "    @step\n",
    "    async def evaluate_initial_description(self, ev: StartEvent) -> InitialDescriptionComplete | InitialDescriptionIncomplete:\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return InitialDescriptionComplete(payload=\"Initial description is complete.\")\n",
    "        else:\n",
    "            return InitialDescriptionIncomplete(payload=\"Initial description is incomplete.\")\n",
    "\n",
    "    @step\n",
    "    async def clarify_initial_description(self, ev: InitialDescriptionIncomplete) -> InitialDescriptionComplete:\n",
    "        return InitialDescriptionComplete(payload=\"Description clarified.\")\n",
    "\n",
    "    @step\n",
    "    async def find_correct_room_type(self, ev: InitialDescriptionComplete | RoomIncorrect | ObjectNotInRoom) -> RoomCorrect | RoomIncorrect:\n",
    "        current_description = \"\"\"a living room setup viewed from behind a dark-colored couch. The room has light-colored walls and a floor that seems to be a muted, earthy tone. The main items in the room include:\n",
    "- A large, dark-colored sofa in the foreground facing a TV.\n",
    "- A television placed on a small white TV stand, positioned along the far wall.\n",
    "- A small side table with a blue vase and a decorative item beside the TV stand.\n",
    "- A wooden shelf or cabinet off to the left side of the room.\"\"\"\n",
    "        \n",
    "        if random.randint(0, 1) == 0:\n",
    "            return RoomCorrect(payload=\"Correct room is found.\")\n",
    "        else:\n",
    "            return RoomIncorrect(payload=\"Correct room is not found.\")\n",
    "\n",
    "    @step \n",
    "    async def find_object_in_room(self, ev: RoomCorrect) -> ObjectInRoom | ObjectNotInRoom:\n",
    "        if random.randint(0, 10) < 4:\n",
    "            return ObjectInRoom(payload=\"Object may be in this room.\")\n",
    "        else:\n",
    "            return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "    \n",
    "    @step \n",
    "    async def suggest_object(self, ev: ObjectInRoom | WrongObjectSuggested) -> WrongObjectSuggested | ObjectNotInRoom | StopEvent:\n",
    "        \n",
    "        \n",
    "        if  random.randint(0, 1) == 0:\n",
    "            return StopEvent(result=\"We found the object!\")  # End the workflow\n",
    "        else:\n",
    "            if random.randint(0, 1) == 0:\n",
    "                return WrongObjectSuggested(payload=\"Couldn't find object in this room.\")\n",
    "            else:\n",
    "                return ObjectNotInRoom(payload=\"Object is not in this room.\")\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# Initialize and run the workflow\n",
    "w = ThorFindsObject(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81d8f70-1c60-4eba-87b3-daf0233f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionComplete'>\n",
      "<class '__main__.InitialDescriptionIncomplete'>\n",
      "<class '__main__.RoomCorrect'>\n",
      "<class '__main__.RoomIncorrect'>\n",
      "<class '__main__.ObjectInRoom'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class '__main__.WrongObjectSuggested'>\n",
      "<class '__main__.ObjectNotInRoom'>\n",
      "<class 'llama_index.core.workflow.events.StopEvent'>\n",
      "possible_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(ThorFindsObject, filename=\"possible_flows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bbad389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chainlit in ./myenv/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2.1)\n",
      "Requirement already satisfied: asyncer<0.0.8,>=0.0.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.7)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (8.1.7)\n",
      "Requirement already satisfied: dataclasses_json<0.7.0,>=0.6.7 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.6.7)\n",
      "Requirement already satisfied: fastapi<0.116,>=0.115.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.115.4)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.27.2)\n",
      "Requirement already satisfied: lazify<0.5.0,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.4.0)\n",
      "Requirement already satisfied: literalai==0.0.623 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.623)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.26 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.2)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.9.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.0.1)\n",
      "Requirement already satisfied: python-multipart<0.0.10,>=0.0.9 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.0.9)\n",
      "Requirement already satisfied: python-socketio<6.0.0,>=5.11.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (5.11.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.41.2 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.41.2)\n",
      "Requirement already satisfied: syncer<3.0.0,>=2.0.3 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in ./myenv/lib/python3.11/site-packages (from chainlit) (2.0.2)\n",
      "Requirement already satisfied: uptrace<2.0.0,>=1.22.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (1.27.0)\n",
      "Requirement already satisfied: uvicorn<0.26.0,>=0.25.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.25.0)\n",
      "Requirement already satisfied: watchfiles<0.21.0,>=0.20.0 in ./myenv/lib/python3.11/site-packages (from chainlit) (0.20.0)\n",
      "Requirement already satisfied: chevron>=0.14.0 in ./myenv/lib/python3.11/site-packages (from literalai==0.0.623->chainlit) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in ./myenv/lib/python3.11/site-packages (from asyncer<0.0.8,>=0.0.7->chainlit) (4.6.2.post1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./myenv/lib/python3.11/site-packages (from dataclasses_json<0.7.0,>=0.6.7->chainlit) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.11/site-packages (from fastapi<0.116,>=0.115.3->chainlit) (4.12.2)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.0.6)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (3.10)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.11/site-packages (from httpx>=0.23.0->chainlit) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./myenv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->chainlit) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./myenv/lib/python3.11/site-packages (from pydantic<3,>=1->chainlit) (2.23.4)\n",
      "Requirement already satisfied: bidict>=0.21.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (0.23.1)\n",
      "Requirement already satisfied: python-engineio>=4.8.0 in ./myenv/lib/python3.11/site-packages (from python-socketio<6.0.0,>=5.11.0->chainlit) (4.10.1)\n",
      "Requirement already satisfied: opentelemetry-api~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation~=0.48b0 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.27 in ./myenv/lib/python3.11/site-packages (from uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.65.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.63.2 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.67.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (1.28.0)\n",
      "Requirement already satisfied: requests~=2.7 in ./myenv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.32.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-proto==1.28.0->opentelemetry-exporter-otlp-proto-grpc==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (5.28.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (0.49b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./myenv/lib/python3.11/site-packages (from opentelemetry-instrumentation~=0.48b0->uptrace<2.0.0,>=1.22.0->chainlit) (1.16.0)\n",
      "Requirement already satisfied: simple-websocket>=0.10.0 in ./myenv/lib/python3.11/site-packages (from python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./myenv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json<0.7.0,>=0.6.7->chainlit) (1.0.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv/lib/python3.11/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.20.2)\n",
      "Requirement already satisfied: wsproto in ./myenv/lib/python3.11/site-packages (from simple-websocket>=0.10.0->python-engineio>=4.8.0->python-socketio<6.0.0,>=5.11.0->chainlit) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.28.0->opentelemetry-exporter-otlp~=1.27->uptrace<2.0.0,>=1.22.0->chainlit) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d5f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
